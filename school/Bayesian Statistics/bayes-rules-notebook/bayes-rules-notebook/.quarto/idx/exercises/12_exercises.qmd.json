{"title":"12: Poisson & Negative Binomial Regression","markdown":{"yaml":{"title":"12: Poisson & Negative Binomial Regression","editor_options":{"chunk_output_type":"console"}},"headingText":"Load packages","containsRefs":false,"markdown":"\n\n```{r setup, warning=FALSE, message=FALSE}\n\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \"#0ACCC5\", \"#CC5514\")\n\n```\n\n# 12.9.1 Conceptual exercises\n\n## Exercise 12.1 (Warming up)\n\na. Give a new example (i.e., not the same as from the chapter) in which we would want to use a Poisson, instead of Normal, regression model. \n   - **Number of children because the distribution is right skewed since most people have 3 or fewer children, few have more than 3, and the mean is probably 1 or 2.**\nb. The Poisson regression model uses a log link function, while the Normal regression model uses an identity link function. Explain in one or two sentences what a link function is. \n   - **Normal -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p, \\sigma \\sim N(\\mu_i, \\sigma^2)$** \n   - **Poisson -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p \\sim Pois(\\lambda_i)$.**\n   - **If $g(\\mu_i) = \\mu_i$ -> *identity link function***\n   - **If $g(\\lambda_i) := \\log(\\lambda_i)$ -> *log link function***\nc. Explain why the log link function is used in Poisson regression.\n   - **When we assume that $\\lambda_i$ can be expressed by a linear combination of the $X$ predictors, the model of $\\lambda_i$ spans both positive and negative values, and thus suggests that some states have a negative number of anti-discrimination laws. That doesn’t make sense. Like the number of laws, a Poisson rate $\\lambda_i$, must be positive. To avoid this violation, it is common to use a log link function. That is, we’ll assume that $log(\\lambda_i)$, which does span both positive and negative values, is a linear combination of the $X$ predictors:** $Y_i | \\beta_0,\\beta_1, \\beta_2, \\beta_3 \\stackrel{ind}{\\sim} Pois\\left(\\lambda_i \\right) \\;\\;\\; \\text{ with } \\;\\;\\; \\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3}$\nd. List the four assumptions for a Poisson regression model.\n   i. **Structure of the data** - Conditioned on predictors $X$, the observed data $Y_i$ on case $i$ is independent of the observed data on any other case $j$.\n   ii. **Structure of variable $Y$** - Response variable $Y$ has a Poisson structure, i.e., is a discrete count of events that happen in a fixed interval of space or time.\n   iii. **Structure of the relationship** - The logged average $Y$ value can be written as a linear combination of the predictors, $\\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}+ \\beta_3 X_{i3}$.\n   iv. **Structure of the variability in $Y$** - A Poisson random variable $Y$ with rate $\\lambda$ has equal mean and variance, $E(Y) = \\text{Var}(Y) = \\lambda$ (5.3). Thus, conditioned on predictors $X$, the typical value of $Y$ should be roughly equivalent to the variability in $Y$. As such, the variability in $Y$ increases as its mean increases. See Figure 12.6 for examples of when this assumption does and does not hold.\n   \n## Exercise 12.2 (Poisson versus Negative Binomial) \n\nSpecify whether Poisson regression, Negative Binomial regression, both, or neither fit with each situation described below.\n\na. The response variable is a count. **both**\nb. The link is a log. **both**\nc. The link is the identity. **neither**\nd. We need to account for overdispersion. **Negative Binomial**\ne. The response is a count variable, and as the expected response increases, the variability also increases. **Poisson**\n\n## Exercise 12.3 (Why use a Negative Binomial) \n\nYou and your friend Nico are in a two-person Bayes Rules! book club. How lovely! Nico has read only part of this chapter, and now they know about Poisson regression, but not Negative Binomial regression. Be a good friend and answer their questions.\n\na. What’s the shortcoming of Poisson regression? **Poisson regression assumes that, at any set of predictor values, the typical value of $Y$ and variability in $Y$ are equivalent. Thus, when $Y$ is overdispersed, i.e., its variability exceeds assumptions, we might instead utilize the more flexible Negative Binomial regression model.**\nb. How does Negative Binomial regression address the shortcoming of Poisson regression? **Poisson regression preserves the Poisson property of equal mean and variance. Negative Binomial regression is more appropriate for high variability relative to a low average.**\nc. Are there any situations in which Poisson regression would be a better choice than Negative Binomial? **When variability is low relative to the average.**\n\n## Exercise 12.4 (Interpreting Poisson regression coefficients) \n\nAs modelers, the ability to interpret regression coefficients is of utmost importance. Let $Y$ be the number of “Likes” a tweet gets in an hour, $X_1$ be the number of followers the person who wrote the tweet has, and $X_2$ indicate whether the tweet includes an emoji ($X_2 = 1$ if there is an emoji, $X_2 = 0$ if there is no emoji). Further, suppose $Y|\\beta_0, \\beta_1, \\beta_2 \\sim \\text{Pois}(\\lambda)$ with\n\n$\\text{log}(\\lambda) = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}  .$\n \na. Interpret $e^{\\beta_0}$ in context. **y-intercept**\nb. Interpret $e^{\\beta_1}$ in context. **slope of followers**\nc. Interpret $e^{\\beta_2}$in context. **slope of emoji**\nd. Provide the model equation for the expected number of “Likes” for a tweet in one hour, when the person who wrote the tweet has 300 followers, and the tweet does not use an emoji. $\\text{log}(\\lambda) = \\beta_0 + ( \\beta_1 * 300 ) + ( \\beta_2 * 0)$.\n\n# 12.9.2 Applied exercises\n\n## Exercise 12.5 (Eagles: get to know the data) \n\nIn the next exercises, you will explore how the number of eagle sightings in Ontario, Canada has changed over time. Since this context is unfamiliar to us, we’ll utilize weakly informative priors throughout. We’ll balance this prior uncertainty by the `bald_eagles` data in the **bayesrules** package, which includes data on bald eagle sightings during 37 different one-week observation periods. First, get to know this data.\n\na. Construct and discuss a univariate plot of `count`, the number of eagle sightings across the observation periods.\n\n```{r}\n\nbald_eagles <- bald_eagles\n\nbald_eagles %>% \n  ggplot(aes(count_per_week)) +\n  geom_histogram(binwidth = 1, fill = \"#0E0ACC\", color = \"white\", boundary = 0)\n\n```\n\nb. Construct and discuss a plot of `count` versus `year.`\n\n```{r, warning=FALSE, message=FALSE}\n\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(size = 4, color = \"#0E0ACC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  theme_light()\n\n```\n\nc. In exploring the number of eagle sightings over time, it’s important to consider the fact that the length of the observation periods vary from year to year, ranging from 134 to 248.75 hours. Update your plot from part b to also include information about the observation length in `hours` and comment on your findings.\n\n```{r, warning=FALSE, message=FALSE}\n\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(aes(fill = hours), size = 4, shape = 21, color = \"#7400CC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  scale_fill_steps(low = \"white\", high = \"#0E0ACC\") +\n  theme_light()\n\n```\n\n## Exercise 12.6 (Eagles: first model attempt) \n\nOur next goal is to model the relationship between bald eagle counts $Y$ by year $X_1$ when controlling for the number of observation hours $X_2$. To begin, consider a Normal regression model of $Y$ versus $X_1$ and $X_2$.\n\na. Simulate the model posterior and check the `prior_summary()`.\n\n```{r}\n\neagle_model_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = gaussian,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n\nprior_summary(eagle_model_prior)\n\n# hi <- bald_eagles %>% \n#   add_epred_draws(eagle_model_prior, ndraws = 50)# %>%\n#   \n# ggplot(hi, aes(year, .epred)) +\n#   geom_line(aes(color = paste(hours, .draw)))\n# \n# ggplot(aes(x = year, y = count, color = hours)) +\n#     geom_line(aes(y = .value, group = paste(hours, .draw))) + \n#   scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\n```\n\nb. Use careful notation to write out the complete Bayesian structure of the Normal regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, \\beta_2, \\sigma & \\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i \\\\\n\\beta_{0c}  & \\sim N\\left(2, .5 \\right)  \\\\\n\\beta_1  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\beta_2  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\sigma   & \\sim \\text{Exp}(1) \\\\\n\\end{aligned}\n\n$$\nc. Complete a `pp_check()` for the Normal model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n```{r}\n\neagle_model <- update(eagle_model_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_model)\nmcmc_dens_overlay(eagle_model)\nmcmc_acf(eagle_model)\n\nset.seed(1)\npp_check(eagle_model) + \n  xlab(\"count\")\n\n```\n\n## Exercise 12.7 (Eagles: second model attempt) \n\nLet’s try to do better. Consider a Poisson regression model of $Y$ versus $X_1$ and $X_2$.\n\na. In the bald eagle analysis, why might a Poisson regression approach be more appropriate than a Normal regression approach?\n\nb. Simulate the posterior of the Poisson regression model of $Y$ versus $X_1$ and $X_2$. Check the `prior_summary()`.\n\n```{r, cache=TRUE}\n\neagle_pois_prior <- stan_glm(count ~ year + hours, \n                             data = bald_eagles, \n                             family = poisson,\n                             prior_intercept = normal(2, 0.5),\n                             prior = normal(0, 2.5, autoscale = TRUE), \n                             chains = 4, iter = 5000*2, seed = 84735, \n                             prior_PD = TRUE)\n\nprior_summary(eagle_pois_prior)\n\nbald_eagles %>% \n  add_fitted_draws(eagle_pois_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\n```\n\nc. Use careful notation to write out the complete Bayesian structure of the Poisson regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nd. Complete a `pp_check()` for the Poisson model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n```{r, cache=TRUE}\n\neagle_pois <- update(eagle_pois_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_pois)\nmcmc_dens_overlay(eagle_pois)\nmcmc_acf(eagle_pois)\n\nset.seed(1)\npp_check(eagle_pois, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\npp_check(eagle_pois) + \n  xlab(\"count\")\n\n```\n\n## Exercise 12.8 (Eagles: an even better model) \n\nThe Poisson regression model of bald eagle counts ($Y$) by year ($X_1$) and observation hours ($X_2$), was pretty good. Let’s see if a Negative Binomial approach is even better.\n\na. Simulate the model posterior and use a `pp_check()` to confirm whether the Negative Binomial model is reasonable.\n\n```{r, cache=TRUE}\n\neagle_negbi_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = neg_binomial_2,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n\nprior_summary(eagle_negbi_prior)\n\nbald_eagles %>% \n  add_fitted_draws(eagle_negbi_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\neagle_negbi <- update(eagle_negbi_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_negbi)\nmcmc_dens_overlay(eagle_negbi)\nmcmc_acf(eagle_negbi)\n\nset.seed(1)\npp_check(eagle_negbi, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\npp_check(eagle_negbi) + \n  xlab(\"count\")\n\n# Numerical summaries\ntidy(eagle_negbi, conf.int = TRUE, conf.level = 0.80)\n\n```\n\nb. Use careful notation to write out the complete Bayesian structure of the Negative Binomial regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nc. Interpret the posterior median estimates of the regression coefficients on `year` and `hours`, $\\beta_1$ and $\\beta_2$. Do so on the unlogged scale.\n   - `year` - $+.0715, .0492-.0945$\n   - `hours` - $+.00457, -.00271/.012$\n   \nd. Construct and interpret a 95% posterior credible interval for the `year` coefficient.\n\ne. When controlling for the number of observation hours, do we have ample evidence that the rate of eagle sightings has increased over time?\n\n## Exercise 12.9 (Eagles: model evaluation) \n\nFinally, let’s evaluate the quality of our Negative Binomial bald eagle model.\n\na. How fair is the model?\nb. How wrong is the model?\nc. How accurate are the model predictions?\n\n## Exercise 12.10 (Open exercise: AirBnB) \n\nThe `airbnb_small` data in the **bayesrules** package contains information on AirBnB rentals in Chicago. This data was originally collated by Trinh and Ameri (2016) and distributed by Legler and Roback (2021). In this open-ended exercise, build, interpret, and evaluate a model of the number of `reviews` an AirBnB property has by its `rating`, `district`, `room_type`, and the number of guests it `accommodates`.\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r setup, warning=FALSE, message=FALSE}\n\n# Load packages\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \"#0ACCC5\", \"#CC5514\")\n\n```\n\n# 12.9.1 Conceptual exercises\n\n## Exercise 12.1 (Warming up)\n\na. Give a new example (i.e., not the same as from the chapter) in which we would want to use a Poisson, instead of Normal, regression model. \n   - **Number of children because the distribution is right skewed since most people have 3 or fewer children, few have more than 3, and the mean is probably 1 or 2.**\nb. The Poisson regression model uses a log link function, while the Normal regression model uses an identity link function. Explain in one or two sentences what a link function is. \n   - **Normal -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p, \\sigma \\sim N(\\mu_i, \\sigma^2)$** \n   - **Poisson -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p \\sim Pois(\\lambda_i)$.**\n   - **If $g(\\mu_i) = \\mu_i$ -> *identity link function***\n   - **If $g(\\lambda_i) := \\log(\\lambda_i)$ -> *log link function***\nc. Explain why the log link function is used in Poisson regression.\n   - **When we assume that $\\lambda_i$ can be expressed by a linear combination of the $X$ predictors, the model of $\\lambda_i$ spans both positive and negative values, and thus suggests that some states have a negative number of anti-discrimination laws. That doesn’t make sense. Like the number of laws, a Poisson rate $\\lambda_i$, must be positive. To avoid this violation, it is common to use a log link function. That is, we’ll assume that $log(\\lambda_i)$, which does span both positive and negative values, is a linear combination of the $X$ predictors:** $Y_i | \\beta_0,\\beta_1, \\beta_2, \\beta_3 \\stackrel{ind}{\\sim} Pois\\left(\\lambda_i \\right) \\;\\;\\; \\text{ with } \\;\\;\\; \\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3}$\nd. List the four assumptions for a Poisson regression model.\n   i. **Structure of the data** - Conditioned on predictors $X$, the observed data $Y_i$ on case $i$ is independent of the observed data on any other case $j$.\n   ii. **Structure of variable $Y$** - Response variable $Y$ has a Poisson structure, i.e., is a discrete count of events that happen in a fixed interval of space or time.\n   iii. **Structure of the relationship** - The logged average $Y$ value can be written as a linear combination of the predictors, $\\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}+ \\beta_3 X_{i3}$.\n   iv. **Structure of the variability in $Y$** - A Poisson random variable $Y$ with rate $\\lambda$ has equal mean and variance, $E(Y) = \\text{Var}(Y) = \\lambda$ (5.3). Thus, conditioned on predictors $X$, the typical value of $Y$ should be roughly equivalent to the variability in $Y$. As such, the variability in $Y$ increases as its mean increases. See Figure 12.6 for examples of when this assumption does and does not hold.\n   \n## Exercise 12.2 (Poisson versus Negative Binomial) \n\nSpecify whether Poisson regression, Negative Binomial regression, both, or neither fit with each situation described below.\n\na. The response variable is a count. **both**\nb. The link is a log. **both**\nc. The link is the identity. **neither**\nd. We need to account for overdispersion. **Negative Binomial**\ne. The response is a count variable, and as the expected response increases, the variability also increases. **Poisson**\n\n## Exercise 12.3 (Why use a Negative Binomial) \n\nYou and your friend Nico are in a two-person Bayes Rules! book club. How lovely! Nico has read only part of this chapter, and now they know about Poisson regression, but not Negative Binomial regression. Be a good friend and answer their questions.\n\na. What’s the shortcoming of Poisson regression? **Poisson regression assumes that, at any set of predictor values, the typical value of $Y$ and variability in $Y$ are equivalent. Thus, when $Y$ is overdispersed, i.e., its variability exceeds assumptions, we might instead utilize the more flexible Negative Binomial regression model.**\nb. How does Negative Binomial regression address the shortcoming of Poisson regression? **Poisson regression preserves the Poisson property of equal mean and variance. Negative Binomial regression is more appropriate for high variability relative to a low average.**\nc. Are there any situations in which Poisson regression would be a better choice than Negative Binomial? **When variability is low relative to the average.**\n\n## Exercise 12.4 (Interpreting Poisson regression coefficients) \n\nAs modelers, the ability to interpret regression coefficients is of utmost importance. Let $Y$ be the number of “Likes” a tweet gets in an hour, $X_1$ be the number of followers the person who wrote the tweet has, and $X_2$ indicate whether the tweet includes an emoji ($X_2 = 1$ if there is an emoji, $X_2 = 0$ if there is no emoji). Further, suppose $Y|\\beta_0, \\beta_1, \\beta_2 \\sim \\text{Pois}(\\lambda)$ with\n\n$\\text{log}(\\lambda) = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}  .$\n \na. Interpret $e^{\\beta_0}$ in context. **y-intercept**\nb. Interpret $e^{\\beta_1}$ in context. **slope of followers**\nc. Interpret $e^{\\beta_2}$in context. **slope of emoji**\nd. Provide the model equation for the expected number of “Likes” for a tweet in one hour, when the person who wrote the tweet has 300 followers, and the tweet does not use an emoji. $\\text{log}(\\lambda) = \\beta_0 + ( \\beta_1 * 300 ) + ( \\beta_2 * 0)$.\n\n# 12.9.2 Applied exercises\n\n## Exercise 12.5 (Eagles: get to know the data) \n\nIn the next exercises, you will explore how the number of eagle sightings in Ontario, Canada has changed over time. Since this context is unfamiliar to us, we’ll utilize weakly informative priors throughout. We’ll balance this prior uncertainty by the `bald_eagles` data in the **bayesrules** package, which includes data on bald eagle sightings during 37 different one-week observation periods. First, get to know this data.\n\na. Construct and discuss a univariate plot of `count`, the number of eagle sightings across the observation periods.\n\n```{r}\n\nbald_eagles <- bald_eagles\n\nbald_eagles %>% \n  ggplot(aes(count_per_week)) +\n  geom_histogram(binwidth = 1, fill = \"#0E0ACC\", color = \"white\", boundary = 0)\n\n```\n\nb. Construct and discuss a plot of `count` versus `year.`\n\n```{r, warning=FALSE, message=FALSE}\n\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(size = 4, color = \"#0E0ACC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  theme_light()\n\n```\n\nc. In exploring the number of eagle sightings over time, it’s important to consider the fact that the length of the observation periods vary from year to year, ranging from 134 to 248.75 hours. Update your plot from part b to also include information about the observation length in `hours` and comment on your findings.\n\n```{r, warning=FALSE, message=FALSE}\n\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(aes(fill = hours), size = 4, shape = 21, color = \"#7400CC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  scale_fill_steps(low = \"white\", high = \"#0E0ACC\") +\n  theme_light()\n\n```\n\n## Exercise 12.6 (Eagles: first model attempt) \n\nOur next goal is to model the relationship between bald eagle counts $Y$ by year $X_1$ when controlling for the number of observation hours $X_2$. To begin, consider a Normal regression model of $Y$ versus $X_1$ and $X_2$.\n\na. Simulate the model posterior and check the `prior_summary()`.\n\n```{r}\n\neagle_model_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = gaussian,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n\nprior_summary(eagle_model_prior)\n\n# hi <- bald_eagles %>% \n#   add_epred_draws(eagle_model_prior, ndraws = 50)# %>%\n#   \n# ggplot(hi, aes(year, .epred)) +\n#   geom_line(aes(color = paste(hours, .draw)))\n# \n# ggplot(aes(x = year, y = count, color = hours)) +\n#     geom_line(aes(y = .value, group = paste(hours, .draw))) + \n#   scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\n```\n\nb. Use careful notation to write out the complete Bayesian structure of the Normal regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, \\beta_2, \\sigma & \\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i \\\\\n\\beta_{0c}  & \\sim N\\left(2, .5 \\right)  \\\\\n\\beta_1  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\beta_2  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\sigma   & \\sim \\text{Exp}(1) \\\\\n\\end{aligned}\n\n$$\nc. Complete a `pp_check()` for the Normal model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n```{r}\n\neagle_model <- update(eagle_model_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_model)\nmcmc_dens_overlay(eagle_model)\nmcmc_acf(eagle_model)\n\nset.seed(1)\npp_check(eagle_model) + \n  xlab(\"count\")\n\n```\n\n## Exercise 12.7 (Eagles: second model attempt) \n\nLet’s try to do better. Consider a Poisson regression model of $Y$ versus $X_1$ and $X_2$.\n\na. In the bald eagle analysis, why might a Poisson regression approach be more appropriate than a Normal regression approach?\n\nb. Simulate the posterior of the Poisson regression model of $Y$ versus $X_1$ and $X_2$. Check the `prior_summary()`.\n\n```{r, cache=TRUE}\n\neagle_pois_prior <- stan_glm(count ~ year + hours, \n                             data = bald_eagles, \n                             family = poisson,\n                             prior_intercept = normal(2, 0.5),\n                             prior = normal(0, 2.5, autoscale = TRUE), \n                             chains = 4, iter = 5000*2, seed = 84735, \n                             prior_PD = TRUE)\n\nprior_summary(eagle_pois_prior)\n\nbald_eagles %>% \n  add_fitted_draws(eagle_pois_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\n```\n\nc. Use careful notation to write out the complete Bayesian structure of the Poisson regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nd. Complete a `pp_check()` for the Poisson model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n```{r, cache=TRUE}\n\neagle_pois <- update(eagle_pois_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_pois)\nmcmc_dens_overlay(eagle_pois)\nmcmc_acf(eagle_pois)\n\nset.seed(1)\npp_check(eagle_pois, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\npp_check(eagle_pois) + \n  xlab(\"count\")\n\n```\n\n## Exercise 12.8 (Eagles: an even better model) \n\nThe Poisson regression model of bald eagle counts ($Y$) by year ($X_1$) and observation hours ($X_2$), was pretty good. Let’s see if a Negative Binomial approach is even better.\n\na. Simulate the model posterior and use a `pp_check()` to confirm whether the Negative Binomial model is reasonable.\n\n```{r, cache=TRUE}\n\neagle_negbi_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = neg_binomial_2,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n\nprior_summary(eagle_negbi_prior)\n\nbald_eagles %>% \n  add_fitted_draws(eagle_negbi_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n\neagle_negbi <- update(eagle_negbi_prior, prior_PD = FALSE)\n\nmcmc_trace(eagle_negbi)\nmcmc_dens_overlay(eagle_negbi)\nmcmc_acf(eagle_negbi)\n\nset.seed(1)\npp_check(eagle_negbi, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\npp_check(eagle_negbi) + \n  xlab(\"count\")\n\n# Numerical summaries\ntidy(eagle_negbi, conf.int = TRUE, conf.level = 0.80)\n\n```\n\nb. Use careful notation to write out the complete Bayesian structure of the Negative Binomial regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nc. Interpret the posterior median estimates of the regression coefficients on `year` and `hours`, $\\beta_1$ and $\\beta_2$. Do so on the unlogged scale.\n   - `year` - $+.0715, .0492-.0945$\n   - `hours` - $+.00457, -.00271/.012$\n   \nd. Construct and interpret a 95% posterior credible interval for the `year` coefficient.\n\ne. When controlling for the number of observation hours, do we have ample evidence that the rate of eagle sightings has increased over time?\n\n## Exercise 12.9 (Eagles: model evaluation) \n\nFinally, let’s evaluate the quality of our Negative Binomial bald eagle model.\n\na. How fair is the model?\nb. How wrong is the model?\nc. How accurate are the model predictions?\n\n## Exercise 12.10 (Open exercise: AirBnB) \n\nThe `airbnb_small` data in the **bayesrules** package contains information on AirBnB rentals in Chicago. This data was originally collated by Trinh and Ameri (2016) and distributed by Legler and Roback (2021). In this open-ended exercise, build, interpret, and evaluate a model of the number of `reviews` an AirBnB property has by its `rating`, `district`, `room_type`, and the number of guests it `accommodates`.\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"12_exercises.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"../custom.scss","title":"12: Poisson & Negative Binomial Regression","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}