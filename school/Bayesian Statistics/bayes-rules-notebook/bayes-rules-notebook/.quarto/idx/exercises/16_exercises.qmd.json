{"title":"16: (Normal) Hierarchical Models without Predictors","markdown":{"yaml":{"title":"16: (Normal) Hierarchical Models without Predictors","editor_options":{"chunk_output_type":"console"}},"headingText":"Load packages","containsRefs":false,"markdown":"\n\n```{r, warning=FALSE, message=FALSE}\n\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\nlibrary(forcats)\nlibrary(patchwork)\nlibrary(ggridges)\n\ndata(climbers_sub)\ndata(coffee_ratings)\n\n```\n\n## Exercise 16.2 (Grouping variable or predictor?)\n\na. The `climbers_sub` data in the `bayesrules` package contains outcomes for 2076 climbers that have sought to summit a Himalayan mountain peak. In a model of climber `success`, is `expedition_id` a potential predictor or a grouping variable? Explain.\n   - **Group because there are many climbers for each `expedition_id`**\nb. In a model of climber `success`, is `season` a potential predictor or a grouping variable? Explain.\n   - **Potential predictor because it may influence success for individual climbers and the expedition as a group**\nc. The `coffee_ratings` data in the `bayesrules` package contains ratings for 1339 different coffee batches. In a model of coffee ratings (`total_cup_points`), is `processing_method` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Group because there are multiple methods for each `farm_name`, or maybe not heirarchical with thses variables**\nd. In a model of coffee ratings (`total_cup_points`), is `farm_name` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Potential predictor because it may influence cup points for individual farms, or maybe not heirarchical with thses variables**\n   \n## Exercise 16.3 (Speed typing: interpret the coefficients) \n\nAlicia loves typing. To share the appreciation, she invites four friends to each take 20 speed-typing tests. Let $Y_{ij}$ be the time it takes friend $j$ to complete test $i$.\n\na. In modeling $Y_{ij}$, explain why it’s important to account for the grouping structure introduced by observing each friend multiple times.\nb. Suppose we were to model the outcomes $Y_{ij}$ by (16.5). Interpret the meaning of all model coefficients in terms of what they might illuminate about typing speeds: $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$.\n   - $\\mu_j =$ mean typing time per person\n   - $\\mu =$ global priors\n   - $\\sigma_y =$ variation within each friend\n   - $\\sigma_\\mu = $ variation between friends\n   \n## Exercise 16.4 (Speed typing: sketch the data) \n\nIn the spirit of Figure 16.8, sketch what density plots of your four friends’ typing speed outcomes $Y_{ij}$ might look like under each scenario below.\n\na. The overall results of the 20 timed tests are remarkably similar among the four friends.\nb. Each person is quite consistent in their typing times, but there are big differences from person to person – some tend to type much faster than others.\nc. Within the subjects, there doesn’t appear to be much correlation in typing time from test to test.\n\n# 16.9.2 Applied exercises\n\n## Exercise 16.6 (Big words: getting to know the data) \n\nRecall from Section 16.7 the Abdul Latif Jameel Poverty Action Lab (J-PAL) study into the effectiveness of a digital vocabulary learning program, the Big Word Club (BWC) (Kalil, Mayer, and Oreopoulos 2020). In our analysis of this program, we’ll utilize weakly informative priors with a baseline understanding that the average student saw 0 change in their vocabulary scores throughout the program. We’ll balance these priors by the `big_word_club` data in the **bayesrules** package. For each student participant, `big_word_club` includes a `school_id` and the *percentage change* in vocabulary scores over the course of the study period (`score_pct_change`). We keep only the students that participated in the BWC program (`treat == 1`), and thus eliminate the control group.\n\n```{r}\n\ndata(\"big_word_club\")\nbig_word_club <- big_word_club %>% \n  filter(treat == 1) %>% \n  select(school_id, score_pct_change) %>% \n  na.omit()\n\n```\n\na. How many schools participated in the Big Word Club? **26**\n\n```{r}\n\nsum <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(mean = mean(score_pct_change))\n\n```\n\nb. What’s the range in the number of student participants per school? **12-17**\n\n```{r}\n\nsum2 <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(n = n())\n\nmin(sum2$n)\nmax(sum2$n)\n\n```\n\nc. On average, at which school did students exhibit the greatest improvement in vocabulary? The least? **17**\n\nd. Construct and discuss a plot which illustrates the variability in `score_pct_change` within and between schools. \n\n```{r}\n#| fig.height: 6\n\nw <- big_word_club %>% \n  ggplot(aes(score_pct_change, fill = school_id)) +\n  geom_density(alpha = .5, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .5) +\n  theme_light()\n\nw / b\n\nw2 <- big_word_club %>% \n  ggplot(aes(score_pct_change, school_id, fill = school_id)) +\n  geom_density_ridges(alpha = .75, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb2 <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .75) +\n  theme_light()\n\nw2 / b2 +\n  plot_layout(heights = c(80, 20))\n\n```\n\n## Exercise 16.7 (Big words: setting up the model) \n\nIn the next exercises you will explore a hierarchical one-way ANOVA model (16.12) of $Y_{ij}$, the percentage change in vocabulary scores, for student $i$ in school $j$.\n\na. Why is a hierarchical model, vs a complete or no pooled model, appropriate in our analysis of the BWC program?\n   - **To account for variation within and between schools**\nb. Compare and contrast the meanings of model parameters $\\mu$ and $\\mu_j$ in the context of this vocabulary study.\n   - $\\mu_j =$ mean score change time per school\n   - $\\mu =$ global priors\nc. Compare and contrast the meanings of model parameters $\\sigma_y$ and $\\sigma_\\mu$ in the context of this vocabulary study.\n   - $\\sigma_y =$ variation within each school\n   - $\\sigma_\\mu = $ variation between schools\n\n## Exercise 16.8 (Big words: simulating the model)\n\na. Simulate the hierarchical posterior model of parameters $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$ using 4 chains, each of length 10000.\n\n```{r, cache=TRUE}\n\nbig_hier <- stan_glmer(\n  score_pct_change ~ (1 | school_id), \n  data = big_word_club , family = gaussian,\n  prior_intercept = normal(5, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = .1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735)\n\n```\n\nb. Construct and discuss Markov chain trace, density, and autocorrelation plots.\n\n```{r}\n\n# Confirm the prior tunings\nprior_summary(big_hier)\n\nmcmc_trace(big_hier)\nmcmc_dens_overlay(big_hier)\nmcmc_acf(big_hier)\nneff_ratio(big_hier)\nrhat(big_hier)\n\n```\n\nc. Construct and discuss a `pp_check()` of the chain output.\n\n```{r}\n\npp_check(big_hier) + \n  xlab(\"score change\")\n\n# Store the simulation in a data frame\nbig_hier_df <- as.data.frame(big_hier)\n\n# Check out the first 3 and last 3 parameter labels\nbig_hier_df %>% \n  colnames() %>% \n  as.data.frame() %>% \n  slice(1:3, 13:14)\n\n```\n\n## Exercise 16.9 (Big words: global parameters) \n\nIn this exercise, we’ll explore the global parameters of our BWC model: $(\\mu,\\sigma_y,\\sigma_\\mu)$.\n\na. Construct and interpret a 95% credible interval for $\\mu$.\n   - **There's a 95% chance that the average school saw an increase between 4.19 and 8.44.**\n\n```{r}\n\ntidy(big_hier, effects = \"fixed\", \n     conf.int = TRUE, conf.level = 0.95)\n\n```\n\nb. Is there ample evidence that, on average, student vocabulary levels improve throughout the vocabulary program? Explain.\n   - **There is ample evidence that, on average, student vocabulary levels improved throughout the vocabulary program by at least 4.19. According to the global parameters of the model 95% of the simulated outcomes were between 4.19 and 8.44.**\nc. Which appears to be larger: the variability in vocabulary performance *between* or *within* schools? Provide posterior evidence and explain the implication of this result in the context of the analysis.\n   - **The variation within schools (16.9) was much higher than the variation between schools (2.88).**\n\n```{r}\n\ntidy(big_hier, effects = \"ran_pars\")\n\n```\n\n## Exercise 16.10 (Big words: focusing on schools) \n\nNext, let’s dig into the school-specific means, $\\mu_j$.\n\na. Construct and discuss a plot of the 80% posterior credible intervals for the average percent change in vocabulary score for all schools in the study, $\\mu_j$.\n\n```{r}\n\nschool_sum <- tidy(big_hier, effects = \"ran_vals\", \n                   conf.int = TRUE, conf.level = 0.80)\n\n# Check out the results for the first & last 2 artists\nschool_sum %>% \n  select(level, conf.low, conf.high) \n\n# Get MCMC chains for each mu_j\nschool_chains <- big_hier %>%\n  spread_draws(`(Intercept)`, b[,school_id]) %>% \n  mutate(mu_j = `(Intercept)` + b) \n\n# Check it out\nschool_chains %>% \n  select(school_id, `(Intercept)`, b, mu_j) %>% \n  head(4)\n\n# Get posterior summaries for mu_j\nschool_sum_scaled <- school_chains %>% \n  select(-`(Intercept)`, -b) %>% \n  mean_qi(.width = 0.80) %>% \n  mutate(school_id = fct_reorder(school_id, mu_j))\n\n# Check out the results\nschool_sum_scaled %>% \n  select(school_id, mu_j, .lower, .upper) %>% \n  head(4)\n\nggplot(school_sum_scaled, \n       aes(school_id, mu_j, ymin = .lower, ymax = .upper)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xaxis_text(angle = 90, hjust = 1) +\n  theme_light()\n\n```\n\nb. Construct and interpret the 80% posterior credible interval for $\\mu_10$.\n\n```{r}\n\nsum %>% \n  filter(school_id %in% c(\"10\"))\n\n# Simulate School 10's posterior predictive model\nset.seed(84735)\nchains10 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:10]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_10 = `(Intercept)` + b,\n         y_10 = rnorm(20000, mean = mu_10, sd = sigma))\n\n# Check it out\nhead(chains10, 3)\n\n# Posterior summary of Y_new,j\nchains10 %>% \n  mean_qi(y_10, .width = 0.80)    \n\n# Posterior summary of mu_j\nschool_sum_scaled %>% \n  filter(school_id == \"school_id:10\")\n\n```\n\nc. Is there ample evidence that, on average, vocabulary scores at School 10 improved by more than 5% throughout the duration of the vocabulary program? Provide posterior evidence.\n   - **Yes because the lower confidence level is 2.36 for school 10.**\n\n## Exercise 16.11 (Big words: predicting vocab levels) \n\nSuppose we *continue* the vocabulary study at each of Schools 6 and 17 (which participated in the current study) and Bayes Prep, a school which is new to the study. In this exercise you’ll make predictions about $Y_{\\text{new},j}$, the vocabulary performance of a student that’s new to the study from each of these three schools $j$.\n\na. *Without* using the `posterior_predict()` shortcut function, simulate posterior predictive models of $Y_{\\text{new},j}$ for School 6 and Bayes Prep. Display the first 6 posterior predictions for both schools.\n\n```{r}\n\nset.seed(84735)\nbp_chains <- big_hier_df %>%\n  mutate(sigma_mu = sqrt(`Sigma[school_id:(Intercept),(Intercept)]`),\n         mu_bp = rnorm(20000, `(Intercept)`, sigma_mu),\n         y_bp = rnorm(20000, mu_bp, sigma))\n\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\n\n# Simulate School 6's posterior predictive model\nset.seed(84735)\nchains6 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:6]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_6 = `(Intercept)` + b,\n         y_6 = rnorm(20000, mean = mu_6, sd = sigma))\n\n# Posterior summary of Y_new,j\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)    \n\nhead(bp_chains, 6)\n\nhead(chains6, 6)\n\n```\n\nb. Using your simulations from part (a), construct, interpret, and compare the 80% posterior predictive intervals of $Y_{\\text{new},j}$ for School 6 and Bayes Prep.\n\n```{r}\n\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)\n\n```\n\nc. Using `posterior_predict()` this time, simulate posterior predictive models of $Y_{\\text{new},j}$ for each of School 6, School 17, and Bayes Prep. Illustrate your simulation results using `mcmc_areas()` and discuss your findings.\n\n```{r}\n\nset.seed(84735)\nprediction_shortcut <- posterior_predict(\n  big_hier,\n  newdata = data.frame(school_id = c(\"6\", \"10\", \"BP\")))\n\n```\n\nd. Finally, construct, plot, and discuss the 80% posterior prediction intervals for all schools in the original study.\n\n```{r}\n\n# Posterior predictive model plots\nmcmc_areas(prediction_shortcut, prob = 0.8) +\n  scale_y_discrete(labels = c(\"6\", \"10\", \"BP\")) +\n  geom_vline(xintercept = 0, color = \"red\")\n\n```\n\n## Exercise 16.12 (Big words: shrinkage) \n\nRe-examine the posterior predictive plots from Exercise 16.11. Would you say that there is a little or a lot of shrinkage in this analysis? Provide evidence and explain why you think this is the case.\n\n```{r}\n\nset.seed(84735)\npredictions_hierarchical <- posterior_predict(big_hier, \n                                              newdata = sum)\n\n# Posterior predictive plots\nppc_intervals(sum$mean, yrep = predictions_hierarchical, \n              prob_outer = 0.80) +\n  scale_x_continuous(labels = sum$school_id, \n                     breaks = 1:nrow(sum)) +\n  xaxis_text(angle = 90, hjust = 1) + \n  geom_hline(yintercept = mean(big_word_club))\n\n```\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r, warning=FALSE, message=FALSE}\n\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\nlibrary(forcats)\nlibrary(patchwork)\nlibrary(ggridges)\n\ndata(climbers_sub)\ndata(coffee_ratings)\n\n```\n\n## Exercise 16.2 (Grouping variable or predictor?)\n\na. The `climbers_sub` data in the `bayesrules` package contains outcomes for 2076 climbers that have sought to summit a Himalayan mountain peak. In a model of climber `success`, is `expedition_id` a potential predictor or a grouping variable? Explain.\n   - **Group because there are many climbers for each `expedition_id`**\nb. In a model of climber `success`, is `season` a potential predictor or a grouping variable? Explain.\n   - **Potential predictor because it may influence success for individual climbers and the expedition as a group**\nc. The `coffee_ratings` data in the `bayesrules` package contains ratings for 1339 different coffee batches. In a model of coffee ratings (`total_cup_points`), is `processing_method` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Group because there are multiple methods for each `farm_name`, or maybe not heirarchical with thses variables**\nd. In a model of coffee ratings (`total_cup_points`), is `farm_name` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Potential predictor because it may influence cup points for individual farms, or maybe not heirarchical with thses variables**\n   \n## Exercise 16.3 (Speed typing: interpret the coefficients) \n\nAlicia loves typing. To share the appreciation, she invites four friends to each take 20 speed-typing tests. Let $Y_{ij}$ be the time it takes friend $j$ to complete test $i$.\n\na. In modeling $Y_{ij}$, explain why it’s important to account for the grouping structure introduced by observing each friend multiple times.\nb. Suppose we were to model the outcomes $Y_{ij}$ by (16.5). Interpret the meaning of all model coefficients in terms of what they might illuminate about typing speeds: $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$.\n   - $\\mu_j =$ mean typing time per person\n   - $\\mu =$ global priors\n   - $\\sigma_y =$ variation within each friend\n   - $\\sigma_\\mu = $ variation between friends\n   \n## Exercise 16.4 (Speed typing: sketch the data) \n\nIn the spirit of Figure 16.8, sketch what density plots of your four friends’ typing speed outcomes $Y_{ij}$ might look like under each scenario below.\n\na. The overall results of the 20 timed tests are remarkably similar among the four friends.\nb. Each person is quite consistent in their typing times, but there are big differences from person to person – some tend to type much faster than others.\nc. Within the subjects, there doesn’t appear to be much correlation in typing time from test to test.\n\n# 16.9.2 Applied exercises\n\n## Exercise 16.6 (Big words: getting to know the data) \n\nRecall from Section 16.7 the Abdul Latif Jameel Poverty Action Lab (J-PAL) study into the effectiveness of a digital vocabulary learning program, the Big Word Club (BWC) (Kalil, Mayer, and Oreopoulos 2020). In our analysis of this program, we’ll utilize weakly informative priors with a baseline understanding that the average student saw 0 change in their vocabulary scores throughout the program. We’ll balance these priors by the `big_word_club` data in the **bayesrules** package. For each student participant, `big_word_club` includes a `school_id` and the *percentage change* in vocabulary scores over the course of the study period (`score_pct_change`). We keep only the students that participated in the BWC program (`treat == 1`), and thus eliminate the control group.\n\n```{r}\n\ndata(\"big_word_club\")\nbig_word_club <- big_word_club %>% \n  filter(treat == 1) %>% \n  select(school_id, score_pct_change) %>% \n  na.omit()\n\n```\n\na. How many schools participated in the Big Word Club? **26**\n\n```{r}\n\nsum <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(mean = mean(score_pct_change))\n\n```\n\nb. What’s the range in the number of student participants per school? **12-17**\n\n```{r}\n\nsum2 <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(n = n())\n\nmin(sum2$n)\nmax(sum2$n)\n\n```\n\nc. On average, at which school did students exhibit the greatest improvement in vocabulary? The least? **17**\n\nd. Construct and discuss a plot which illustrates the variability in `score_pct_change` within and between schools. \n\n```{r}\n#| fig.height: 6\n\nw <- big_word_club %>% \n  ggplot(aes(score_pct_change, fill = school_id)) +\n  geom_density(alpha = .5, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .5) +\n  theme_light()\n\nw / b\n\nw2 <- big_word_club %>% \n  ggplot(aes(score_pct_change, school_id, fill = school_id)) +\n  geom_density_ridges(alpha = .75, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb2 <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .75) +\n  theme_light()\n\nw2 / b2 +\n  plot_layout(heights = c(80, 20))\n\n```\n\n## Exercise 16.7 (Big words: setting up the model) \n\nIn the next exercises you will explore a hierarchical one-way ANOVA model (16.12) of $Y_{ij}$, the percentage change in vocabulary scores, for student $i$ in school $j$.\n\na. Why is a hierarchical model, vs a complete or no pooled model, appropriate in our analysis of the BWC program?\n   - **To account for variation within and between schools**\nb. Compare and contrast the meanings of model parameters $\\mu$ and $\\mu_j$ in the context of this vocabulary study.\n   - $\\mu_j =$ mean score change time per school\n   - $\\mu =$ global priors\nc. Compare and contrast the meanings of model parameters $\\sigma_y$ and $\\sigma_\\mu$ in the context of this vocabulary study.\n   - $\\sigma_y =$ variation within each school\n   - $\\sigma_\\mu = $ variation between schools\n\n## Exercise 16.8 (Big words: simulating the model)\n\na. Simulate the hierarchical posterior model of parameters $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$ using 4 chains, each of length 10000.\n\n```{r, cache=TRUE}\n\nbig_hier <- stan_glmer(\n  score_pct_change ~ (1 | school_id), \n  data = big_word_club , family = gaussian,\n  prior_intercept = normal(5, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = .1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735)\n\n```\n\nb. Construct and discuss Markov chain trace, density, and autocorrelation plots.\n\n```{r}\n\n# Confirm the prior tunings\nprior_summary(big_hier)\n\nmcmc_trace(big_hier)\nmcmc_dens_overlay(big_hier)\nmcmc_acf(big_hier)\nneff_ratio(big_hier)\nrhat(big_hier)\n\n```\n\nc. Construct and discuss a `pp_check()` of the chain output.\n\n```{r}\n\npp_check(big_hier) + \n  xlab(\"score change\")\n\n# Store the simulation in a data frame\nbig_hier_df <- as.data.frame(big_hier)\n\n# Check out the first 3 and last 3 parameter labels\nbig_hier_df %>% \n  colnames() %>% \n  as.data.frame() %>% \n  slice(1:3, 13:14)\n\n```\n\n## Exercise 16.9 (Big words: global parameters) \n\nIn this exercise, we’ll explore the global parameters of our BWC model: $(\\mu,\\sigma_y,\\sigma_\\mu)$.\n\na. Construct and interpret a 95% credible interval for $\\mu$.\n   - **There's a 95% chance that the average school saw an increase between 4.19 and 8.44.**\n\n```{r}\n\ntidy(big_hier, effects = \"fixed\", \n     conf.int = TRUE, conf.level = 0.95)\n\n```\n\nb. Is there ample evidence that, on average, student vocabulary levels improve throughout the vocabulary program? Explain.\n   - **There is ample evidence that, on average, student vocabulary levels improved throughout the vocabulary program by at least 4.19. According to the global parameters of the model 95% of the simulated outcomes were between 4.19 and 8.44.**\nc. Which appears to be larger: the variability in vocabulary performance *between* or *within* schools? Provide posterior evidence and explain the implication of this result in the context of the analysis.\n   - **The variation within schools (16.9) was much higher than the variation between schools (2.88).**\n\n```{r}\n\ntidy(big_hier, effects = \"ran_pars\")\n\n```\n\n## Exercise 16.10 (Big words: focusing on schools) \n\nNext, let’s dig into the school-specific means, $\\mu_j$.\n\na. Construct and discuss a plot of the 80% posterior credible intervals for the average percent change in vocabulary score for all schools in the study, $\\mu_j$.\n\n```{r}\n\nschool_sum <- tidy(big_hier, effects = \"ran_vals\", \n                   conf.int = TRUE, conf.level = 0.80)\n\n# Check out the results for the first & last 2 artists\nschool_sum %>% \n  select(level, conf.low, conf.high) \n\n# Get MCMC chains for each mu_j\nschool_chains <- big_hier %>%\n  spread_draws(`(Intercept)`, b[,school_id]) %>% \n  mutate(mu_j = `(Intercept)` + b) \n\n# Check it out\nschool_chains %>% \n  select(school_id, `(Intercept)`, b, mu_j) %>% \n  head(4)\n\n# Get posterior summaries for mu_j\nschool_sum_scaled <- school_chains %>% \n  select(-`(Intercept)`, -b) %>% \n  mean_qi(.width = 0.80) %>% \n  mutate(school_id = fct_reorder(school_id, mu_j))\n\n# Check out the results\nschool_sum_scaled %>% \n  select(school_id, mu_j, .lower, .upper) %>% \n  head(4)\n\nggplot(school_sum_scaled, \n       aes(school_id, mu_j, ymin = .lower, ymax = .upper)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xaxis_text(angle = 90, hjust = 1) +\n  theme_light()\n\n```\n\nb. Construct and interpret the 80% posterior credible interval for $\\mu_10$.\n\n```{r}\n\nsum %>% \n  filter(school_id %in% c(\"10\"))\n\n# Simulate School 10's posterior predictive model\nset.seed(84735)\nchains10 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:10]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_10 = `(Intercept)` + b,\n         y_10 = rnorm(20000, mean = mu_10, sd = sigma))\n\n# Check it out\nhead(chains10, 3)\n\n# Posterior summary of Y_new,j\nchains10 %>% \n  mean_qi(y_10, .width = 0.80)    \n\n# Posterior summary of mu_j\nschool_sum_scaled %>% \n  filter(school_id == \"school_id:10\")\n\n```\n\nc. Is there ample evidence that, on average, vocabulary scores at School 10 improved by more than 5% throughout the duration of the vocabulary program? Provide posterior evidence.\n   - **Yes because the lower confidence level is 2.36 for school 10.**\n\n## Exercise 16.11 (Big words: predicting vocab levels) \n\nSuppose we *continue* the vocabulary study at each of Schools 6 and 17 (which participated in the current study) and Bayes Prep, a school which is new to the study. In this exercise you’ll make predictions about $Y_{\\text{new},j}$, the vocabulary performance of a student that’s new to the study from each of these three schools $j$.\n\na. *Without* using the `posterior_predict()` shortcut function, simulate posterior predictive models of $Y_{\\text{new},j}$ for School 6 and Bayes Prep. Display the first 6 posterior predictions for both schools.\n\n```{r}\n\nset.seed(84735)\nbp_chains <- big_hier_df %>%\n  mutate(sigma_mu = sqrt(`Sigma[school_id:(Intercept),(Intercept)]`),\n         mu_bp = rnorm(20000, `(Intercept)`, sigma_mu),\n         y_bp = rnorm(20000, mu_bp, sigma))\n\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\n\n# Simulate School 6's posterior predictive model\nset.seed(84735)\nchains6 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:6]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_6 = `(Intercept)` + b,\n         y_6 = rnorm(20000, mean = mu_6, sd = sigma))\n\n# Posterior summary of Y_new,j\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)    \n\nhead(bp_chains, 6)\n\nhead(chains6, 6)\n\n```\n\nb. Using your simulations from part (a), construct, interpret, and compare the 80% posterior predictive intervals of $Y_{\\text{new},j}$ for School 6 and Bayes Prep.\n\n```{r}\n\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)\n\n```\n\nc. Using `posterior_predict()` this time, simulate posterior predictive models of $Y_{\\text{new},j}$ for each of School 6, School 17, and Bayes Prep. Illustrate your simulation results using `mcmc_areas()` and discuss your findings.\n\n```{r}\n\nset.seed(84735)\nprediction_shortcut <- posterior_predict(\n  big_hier,\n  newdata = data.frame(school_id = c(\"6\", \"10\", \"BP\")))\n\n```\n\nd. Finally, construct, plot, and discuss the 80% posterior prediction intervals for all schools in the original study.\n\n```{r}\n\n# Posterior predictive model plots\nmcmc_areas(prediction_shortcut, prob = 0.8) +\n  scale_y_discrete(labels = c(\"6\", \"10\", \"BP\")) +\n  geom_vline(xintercept = 0, color = \"red\")\n\n```\n\n## Exercise 16.12 (Big words: shrinkage) \n\nRe-examine the posterior predictive plots from Exercise 16.11. Would you say that there is a little or a lot of shrinkage in this analysis? Provide evidence and explain why you think this is the case.\n\n```{r}\n\nset.seed(84735)\npredictions_hierarchical <- posterior_predict(big_hier, \n                                              newdata = sum)\n\n# Posterior predictive plots\nppc_intervals(sum$mean, yrep = predictions_hierarchical, \n              prob_outer = 0.80) +\n  scale_x_continuous(labels = sum$school_id, \n                     breaks = 1:nrow(sum)) +\n  xaxis_text(angle = 90, hjust = 1) + \n  geom_hline(yintercept = mean(big_word_club))\n\n```\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"16_exercises.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"../custom.scss","title":"16: (Normal) Hierarchical Models without Predictors","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}