{"title":"11: Extending the Normal Regression Model","markdown":{"yaml":{"title":"11: Extending the Normal Regression Model","editor_options":{"chunk_output_type":"console"}},"headingText":"Load some packages","containsRefs":false,"markdown":"\n\n```{r setup, warning=FALSE, message=FALSE}\n\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(tidybayes)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \n            \"#CC5514\", \"#0ACCC5\")\n\n```\n\n## Exercise 11.1 (Why multiple predictors?) \n\nBriefly explain why we might want to build a regression model with more than one predictor.\n\n## Exercise 11.2 (Categorical predictors: cars) \n\nLet’s say that you want to model a car’s miles per gallon in a city ($Y$) by the make of the car: Ford, Kia, Subaru, or Toyota. This relationship can be written as $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+\\beta_3X_{3}$, where $X_1, X_2, X_3$ are indicators for whether or not the cars are Kias, Subarus, or Toyotas, respectively:\n\na. Explain why there is no indicator term for the Ford category. \n   - **Ford is the reference group**\nb. Interpret the regression coefficient $\\beta_2$. \n   - **The expected change in MPG if the car is a Subaru**\nc. Interpret the regression coefficient $\\beta_0$. \n   - **The mean MPG if the car is a Ford**\n\n## Exercise 11.3 (Categorical and quantitative predictors: tomatoes) \n\nYou have recently taken up the hobby of growing tomatoes and hope to learn more about the factors associated with bigger tomatoes. As such, you plan to model a tomato’s weight in grams ($Y$) by the number of days it has been growing ($X_1$) and its type, Mr. Stripey or Roma. Suppose the expected weight of a tomato is a linear function of its age and type, $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}$, where $X_2$ is an indicator for Roma tomatoes.\n\na. Interpret each regression coefficient, $\\beta_0$, $\\beta_1$, and $\\beta_2$.\n   i. **$\\beta_0 =$ y-intercept (expected mean of Mr. Stripey tomatoes and zero days)** \n   ii. **$\\beta_1 =$ expected change for Mr. Stripey tomatoes with each additional day**\n   iii. **$\\beta_2 =$ expected change if Roma tomatoes if days are held constant**\nb. What would it mean if $\\beta_2$ were equal to zero?\n   - **Mr. Stripey tomatoes (reference group)**\n\n## Exercise 11.4 (Interactions: tomatoes) \n\nContinuing your quest to understand tomato size, you incorporate an interaction term between the tomato grow time ($X_1$) and type ($X_2$) into your model: $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+ \\beta_3X_{1}X_{2}$.\n\na. Explain, in context, what it means for $X_1$ and $X_2$ to interact.\n   - **That there is an added effect when $X_1$ and $X_2$ both change in addition to the effect of only one changing**\nb. Interpret $\\beta_3$.\n   - **$\\beta_3 =$ the additional expected change when tomatoes are Roma and increase by one day**\n\n## Exercise 11.5 (Interaction terms)\n\na. Sketch a model that would benefit from including an interaction term between a categorical and quantitative predictor.\nb. Sketch a model that would not benefit from including an interaction term between a categorical and quantitative predictor.\nc. Besides visualization, what are two other ways to determine if you should include interaction terms in your model?\n\n## Exercise 11.6 (Improving your model: shoe size) \n\nLet’s say you model a child’s shoe size ($Y$) by two predictors: the child’s age in years ($X_1$) and an indicator of whether the child knows how to swim ($X_2$).\n\na. Generally speaking, why can it be beneficial to add predictors to models?\n   - **To achieve a better comparison across similar groups**\nb. Generally speaking, why can it be beneficial to remove predictors from models?\n   - **Too many predictors can hide the strength of correlations by attributing it to another (i.e. higher age = more likely to swim, making swimming look like a predictor but truly it is age)**\nc. What might you *add* to this model to improve your predictions of shoe size? Why?\n   - **Gender, since boys and girls may have different average show size by gender**\nd. What might you *remove* from this model to improve it? Why?\n   - **Ability to swim since age probably does a better job of predicting shoe size than ability to swim**\n\n## Exercise 11.7 (What makes a good model?) \n\nWe don’t expect our regression models to be perfect, but we do want to do our best. It can be helpful to think about what we want and expect from our models.\n\na. What are qualities of a good model?\n   - **Enough predictors to explain any confounding factors**\nb. What are qualities of a bad model?\n   - **Too many or too few predictors that over or under inflate correlations**\n\n## Exercise 11.8 (Is our model good / better?) \n\nWhat techniques have you learned in this chapter to assess and compare your models? Give a brief explanation for each technique.\n\n## Exercise 11.9 (Bias-variance trade-off) \n\nIn your own words, briefly explain what the bias-variance tradeoff is and why it is important.\n\n\n# 11.7.2 Applied exercises\n\nIn the next exercises you will use the `penguins_bayes` data in the **bayesrules** package to build various models of penguin `body_mass_g.` Throughout, we’ll utilize weakly informative priors and a basic understanding that the average penguin weighs somewhere between 3,500 and 4,500 grams. Further, one predictor of interest is penguin `species`: Adelie, Chinstrap, or Gentoo. We’ll get our first experience with a 3-level predictor like this in Chapter 12. If you’d like to work with only 2 levels as you did in Chapter 11, you can utilize the `penguin_data` which includes only Adelie and Gentoo penguins:\n\n```{r}\n\n# Alternative penguin data\npenguin_data <- penguins_bayes %>% \n  filter(species %in% c(\"Adelie\", \"Gentoo\"))\n\n```\n\n## Exercise 11.10 (Penguins! Main effects) \n\nLet’s begin our analysis of penguin `body_mass_g` by exploring its relationship with `flipper_length_mm` and `species`.\n\na. Plot and summarize the observed relationships among these three variables.\n\n```{r}\n\npenguin_data %>% \n  ggplot(aes(body_mass_g, flipper_length_mm, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(values = colors) +\n  theme_light()\n\npenguin_data %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors) +\n  theme_light()\n\nsum <- penguin_data %>% \n  drop_na(body_mass_g, flipper_length_mm) %>% \n  group_by(species) %>% \n  summarise(bm = mean(body_mass_g),\n            fl = mean(flipper_length_mm))\n\nsum\n\n```\n\nb. Use `stan_glm()` to simulate a posterior Normal regression model of `body_mass_g` by `flipper_length_mm` and `species`, without an interaction term.\n\n```{r, cache=TRUE}\n\npeng_model <- stan_glm(body_mass_g ~ flipper_length_mm + species, \n                       data = penguin_data, \n                       family = gaussian,\n                       prior_intercept = normal(4000, 250),\n                       prior = normal(0, 2.5, autoscale = TRUE), \n                       prior_aux = exponential(1, autoscale = TRUE),\n                       chains = 4, iter = 5000*2, seed = 84735,\n                       prior_PD = TRUE)\n\n\n```\n\nc. Create and interpret both visual and numerical diagnostics of your MCMC simulation.\n\n```{r}\n\nset.seed(84735)\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_predicted_draws(peng_model, n = 100) %>%\n  ggplot(aes(x = .prediction, group = .draw)) +\n    geom_density() + \n    xlab(\"body_mass_g\")\n\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_fitted_draws(peng_model, n = 100) %>%\n  ggplot(aes(flipper_length_mm, body_mass_g, color = species)) +\n    geom_line(aes(y = .value, group = paste(species, .draw)))\n\n# MCMC diagnostics\nmcmc_trace(peng_model, size = 0.1)\nmcmc_dens_overlay(peng_model)\nmcmc_acf(peng_model)\n\n```\n\nd. Produce a `tidy()` summary of this model. Interpret the non-intercept coefficients’ posterior median values in context.\n\n```{r}\n\n# Posterior summary statistics\npeng_tidy <- tidy(peng_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %>% \n  select(-std.error)\n\npeng_tidy\n\nas.data.frame(peng_model) %>% \n  mutate(adelie = `(Intercept)`, \n         gentoo = `(Intercept)` + speciesGentoo) %>% \n  mcmc_areas(pars = c(\"adelie\", \"gentoo\"))\n\n```\n\ne. Simulate, plot, and describe the posterior predictive model for the body mass of an Adelie penguin that has a flipper length of 197.\n\n```{r}\n\n# Simulate a set of predictions\nset.seed(84735)\nbm_predict <- posterior_predict(peng_model,\n                                newdata = data.frame(flipper_length_mm = 197, \n                                                     species = \"Adelie\"))\nmean(bm_predict)\n\n# Plot the posterior predictive models\nmcmc_areas(bm_predict) +\n  scale_y_discrete(labels = \"Adelie\") + \n  xlab(\"body_mass_g\")\n\n```\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r setup, warning=FALSE, message=FALSE}\n\n# Load some packages\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(tidybayes)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \n            \"#CC5514\", \"#0ACCC5\")\n\n```\n\n## Exercise 11.1 (Why multiple predictors?) \n\nBriefly explain why we might want to build a regression model with more than one predictor.\n\n## Exercise 11.2 (Categorical predictors: cars) \n\nLet’s say that you want to model a car’s miles per gallon in a city ($Y$) by the make of the car: Ford, Kia, Subaru, or Toyota. This relationship can be written as $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+\\beta_3X_{3}$, where $X_1, X_2, X_3$ are indicators for whether or not the cars are Kias, Subarus, or Toyotas, respectively:\n\na. Explain why there is no indicator term for the Ford category. \n   - **Ford is the reference group**\nb. Interpret the regression coefficient $\\beta_2$. \n   - **The expected change in MPG if the car is a Subaru**\nc. Interpret the regression coefficient $\\beta_0$. \n   - **The mean MPG if the car is a Ford**\n\n## Exercise 11.3 (Categorical and quantitative predictors: tomatoes) \n\nYou have recently taken up the hobby of growing tomatoes and hope to learn more about the factors associated with bigger tomatoes. As such, you plan to model a tomato’s weight in grams ($Y$) by the number of days it has been growing ($X_1$) and its type, Mr. Stripey or Roma. Suppose the expected weight of a tomato is a linear function of its age and type, $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}$, where $X_2$ is an indicator for Roma tomatoes.\n\na. Interpret each regression coefficient, $\\beta_0$, $\\beta_1$, and $\\beta_2$.\n   i. **$\\beta_0 =$ y-intercept (expected mean of Mr. Stripey tomatoes and zero days)** \n   ii. **$\\beta_1 =$ expected change for Mr. Stripey tomatoes with each additional day**\n   iii. **$\\beta_2 =$ expected change if Roma tomatoes if days are held constant**\nb. What would it mean if $\\beta_2$ were equal to zero?\n   - **Mr. Stripey tomatoes (reference group)**\n\n## Exercise 11.4 (Interactions: tomatoes) \n\nContinuing your quest to understand tomato size, you incorporate an interaction term between the tomato grow time ($X_1$) and type ($X_2$) into your model: $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+ \\beta_3X_{1}X_{2}$.\n\na. Explain, in context, what it means for $X_1$ and $X_2$ to interact.\n   - **That there is an added effect when $X_1$ and $X_2$ both change in addition to the effect of only one changing**\nb. Interpret $\\beta_3$.\n   - **$\\beta_3 =$ the additional expected change when tomatoes are Roma and increase by one day**\n\n## Exercise 11.5 (Interaction terms)\n\na. Sketch a model that would benefit from including an interaction term between a categorical and quantitative predictor.\nb. Sketch a model that would not benefit from including an interaction term between a categorical and quantitative predictor.\nc. Besides visualization, what are two other ways to determine if you should include interaction terms in your model?\n\n## Exercise 11.6 (Improving your model: shoe size) \n\nLet’s say you model a child’s shoe size ($Y$) by two predictors: the child’s age in years ($X_1$) and an indicator of whether the child knows how to swim ($X_2$).\n\na. Generally speaking, why can it be beneficial to add predictors to models?\n   - **To achieve a better comparison across similar groups**\nb. Generally speaking, why can it be beneficial to remove predictors from models?\n   - **Too many predictors can hide the strength of correlations by attributing it to another (i.e. higher age = more likely to swim, making swimming look like a predictor but truly it is age)**\nc. What might you *add* to this model to improve your predictions of shoe size? Why?\n   - **Gender, since boys and girls may have different average show size by gender**\nd. What might you *remove* from this model to improve it? Why?\n   - **Ability to swim since age probably does a better job of predicting shoe size than ability to swim**\n\n## Exercise 11.7 (What makes a good model?) \n\nWe don’t expect our regression models to be perfect, but we do want to do our best. It can be helpful to think about what we want and expect from our models.\n\na. What are qualities of a good model?\n   - **Enough predictors to explain any confounding factors**\nb. What are qualities of a bad model?\n   - **Too many or too few predictors that over or under inflate correlations**\n\n## Exercise 11.8 (Is our model good / better?) \n\nWhat techniques have you learned in this chapter to assess and compare your models? Give a brief explanation for each technique.\n\n## Exercise 11.9 (Bias-variance trade-off) \n\nIn your own words, briefly explain what the bias-variance tradeoff is and why it is important.\n\n\n# 11.7.2 Applied exercises\n\nIn the next exercises you will use the `penguins_bayes` data in the **bayesrules** package to build various models of penguin `body_mass_g.` Throughout, we’ll utilize weakly informative priors and a basic understanding that the average penguin weighs somewhere between 3,500 and 4,500 grams. Further, one predictor of interest is penguin `species`: Adelie, Chinstrap, or Gentoo. We’ll get our first experience with a 3-level predictor like this in Chapter 12. If you’d like to work with only 2 levels as you did in Chapter 11, you can utilize the `penguin_data` which includes only Adelie and Gentoo penguins:\n\n```{r}\n\n# Alternative penguin data\npenguin_data <- penguins_bayes %>% \n  filter(species %in% c(\"Adelie\", \"Gentoo\"))\n\n```\n\n## Exercise 11.10 (Penguins! Main effects) \n\nLet’s begin our analysis of penguin `body_mass_g` by exploring its relationship with `flipper_length_mm` and `species`.\n\na. Plot and summarize the observed relationships among these three variables.\n\n```{r}\n\npenguin_data %>% \n  ggplot(aes(body_mass_g, flipper_length_mm, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(values = colors) +\n  theme_light()\n\npenguin_data %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors) +\n  theme_light()\n\nsum <- penguin_data %>% \n  drop_na(body_mass_g, flipper_length_mm) %>% \n  group_by(species) %>% \n  summarise(bm = mean(body_mass_g),\n            fl = mean(flipper_length_mm))\n\nsum\n\n```\n\nb. Use `stan_glm()` to simulate a posterior Normal regression model of `body_mass_g` by `flipper_length_mm` and `species`, without an interaction term.\n\n```{r, cache=TRUE}\n\npeng_model <- stan_glm(body_mass_g ~ flipper_length_mm + species, \n                       data = penguin_data, \n                       family = gaussian,\n                       prior_intercept = normal(4000, 250),\n                       prior = normal(0, 2.5, autoscale = TRUE), \n                       prior_aux = exponential(1, autoscale = TRUE),\n                       chains = 4, iter = 5000*2, seed = 84735,\n                       prior_PD = TRUE)\n\n\n```\n\nc. Create and interpret both visual and numerical diagnostics of your MCMC simulation.\n\n```{r}\n\nset.seed(84735)\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_predicted_draws(peng_model, n = 100) %>%\n  ggplot(aes(x = .prediction, group = .draw)) +\n    geom_density() + \n    xlab(\"body_mass_g\")\n\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_fitted_draws(peng_model, n = 100) %>%\n  ggplot(aes(flipper_length_mm, body_mass_g, color = species)) +\n    geom_line(aes(y = .value, group = paste(species, .draw)))\n\n# MCMC diagnostics\nmcmc_trace(peng_model, size = 0.1)\nmcmc_dens_overlay(peng_model)\nmcmc_acf(peng_model)\n\n```\n\nd. Produce a `tidy()` summary of this model. Interpret the non-intercept coefficients’ posterior median values in context.\n\n```{r}\n\n# Posterior summary statistics\npeng_tidy <- tidy(peng_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %>% \n  select(-std.error)\n\npeng_tidy\n\nas.data.frame(peng_model) %>% \n  mutate(adelie = `(Intercept)`, \n         gentoo = `(Intercept)` + speciesGentoo) %>% \n  mcmc_areas(pars = c(\"adelie\", \"gentoo\"))\n\n```\n\ne. Simulate, plot, and describe the posterior predictive model for the body mass of an Adelie penguin that has a flipper length of 197.\n\n```{r}\n\n# Simulate a set of predictions\nset.seed(84735)\nbm_predict <- posterior_predict(peng_model,\n                                newdata = data.frame(flipper_length_mm = 197, \n                                                     species = \"Adelie\"))\nmean(bm_predict)\n\n# Plot the posterior predictive models\nmcmc_areas(bm_predict) +\n  scale_y_discrete(labels = \"Adelie\") + \n  xlab(\"body_mass_g\")\n\n```\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"11_exercises.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"../custom.scss","title":"11: Extending the Normal Regression Model","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}