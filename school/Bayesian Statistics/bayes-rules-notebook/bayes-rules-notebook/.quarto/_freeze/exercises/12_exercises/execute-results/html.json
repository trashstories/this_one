{
  "hash": "9f39d0153a0b52b00b998e202fc744e1",
  "result": {
    "markdown": "---\ntitle: \"12: Poisson & Negative Binomial Regression\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \"#0ACCC5\", \"#CC5514\")\n```\n:::\n\n\n# 12.9.1 Conceptual exercises\n\n## Exercise 12.1 (Warming up)\n\na. Give a new example (i.e., not the same as from the chapter) in which we would want to use a Poisson, instead of Normal, regression model. \n   - **Number of children because the distribution is right skewed since most people have 3 or fewer children, few have more than 3, and the mean is probably 1 or 2.**\nb. The Poisson regression model uses a log link function, while the Normal regression model uses an identity link function. Explain in one or two sentences what a link function is. \n   - **Normal -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p, \\sigma \\sim N(\\mu_i, \\sigma^2)$** \n   - **Poisson -> $Y_i | \\beta_0, \\beta_1, \\cdots, \\beta_p \\sim Pois(\\lambda_i)$.**\n   - **If $g(\\mu_i) = \\mu_i$ -> *identity link function***\n   - **If $g(\\lambda_i) := \\log(\\lambda_i)$ -> *log link function***\nc. Explain why the log link function is used in Poisson regression.\n   - **When we assume that $\\lambda_i$ can be expressed by a linear combination of the $X$ predictors, the model of $\\lambda_i$ spans both positive and negative values, and thus suggests that some states have a negative number of anti-discrimination laws. That doesn’t make sense. Like the number of laws, a Poisson rate $\\lambda_i$, must be positive. To avoid this violation, it is common to use a log link function. That is, we’ll assume that $log(\\lambda_i)$, which does span both positive and negative values, is a linear combination of the $X$ predictors:** $Y_i | \\beta_0,\\beta_1, \\beta_2, \\beta_3 \\stackrel{ind}{\\sim} Pois\\left(\\lambda_i \\right) \\;\\;\\; \\text{ with } \\;\\;\\; \\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3}$\nd. List the four assumptions for a Poisson regression model.\n   i. **Structure of the data** - Conditioned on predictors $X$, the observed data $Y_i$ on case $i$ is independent of the observed data on any other case $j$.\n   ii. **Structure of variable $Y$** - Response variable $Y$ has a Poisson structure, i.e., is a discrete count of events that happen in a fixed interval of space or time.\n   iii. **Structure of the relationship** - The logged average $Y$ value can be written as a linear combination of the predictors, $\\log\\left( \\lambda_i \\right) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}+ \\beta_3 X_{i3}$.\n   iv. **Structure of the variability in $Y$** - A Poisson random variable $Y$ with rate $\\lambda$ has equal mean and variance, $E(Y) = \\text{Var}(Y) = \\lambda$ (5.3). Thus, conditioned on predictors $X$, the typical value of $Y$ should be roughly equivalent to the variability in $Y$. As such, the variability in $Y$ increases as its mean increases. See Figure 12.6 for examples of when this assumption does and does not hold.\n   \n## Exercise 12.2 (Poisson versus Negative Binomial) \n\nSpecify whether Poisson regression, Negative Binomial regression, both, or neither fit with each situation described below.\n\na. The response variable is a count. **both**\nb. The link is a log. **both**\nc. The link is the identity. **neither**\nd. We need to account for overdispersion. **Negative Binomial**\ne. The response is a count variable, and as the expected response increases, the variability also increases. **Poisson**\n\n## Exercise 12.3 (Why use a Negative Binomial) \n\nYou and your friend Nico are in a two-person Bayes Rules! book club. How lovely! Nico has read only part of this chapter, and now they know about Poisson regression, but not Negative Binomial regression. Be a good friend and answer their questions.\n\na. What’s the shortcoming of Poisson regression? **Poisson regression assumes that, at any set of predictor values, the typical value of $Y$ and variability in $Y$ are equivalent. Thus, when $Y$ is overdispersed, i.e., its variability exceeds assumptions, we might instead utilize the more flexible Negative Binomial regression model.**\nb. How does Negative Binomial regression address the shortcoming of Poisson regression? **Poisson regression preserves the Poisson property of equal mean and variance. Negative Binomial regression is more appropriate for high variability relative to a low average.**\nc. Are there any situations in which Poisson regression would be a better choice than Negative Binomial? **When variability is low relative to the average.**\n\n## Exercise 12.4 (Interpreting Poisson regression coefficients) \n\nAs modelers, the ability to interpret regression coefficients is of utmost importance. Let $Y$ be the number of “Likes” a tweet gets in an hour, $X_1$ be the number of followers the person who wrote the tweet has, and $X_2$ indicate whether the tweet includes an emoji ($X_2 = 1$ if there is an emoji, $X_2 = 0$ if there is no emoji). Further, suppose $Y|\\beta_0, \\beta_1, \\beta_2 \\sim \\text{Pois}(\\lambda)$ with\n\n$\\text{log}(\\lambda) = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}  .$\n \na. Interpret $e^{\\beta_0}$ in context. **y-intercept**\nb. Interpret $e^{\\beta_1}$ in context. **slope of followers**\nc. Interpret $e^{\\beta_2}$in context. **slope of emoji**\nd. Provide the model equation for the expected number of “Likes” for a tweet in one hour, when the person who wrote the tweet has 300 followers, and the tweet does not use an emoji. $\\text{log}(\\lambda) = \\beta_0 + ( \\beta_1 * 300 ) + ( \\beta_2 * 0)$.\n\n# 12.9.2 Applied exercises\n\n## Exercise 12.5 (Eagles: get to know the data) \n\nIn the next exercises, you will explore how the number of eagle sightings in Ontario, Canada has changed over time. Since this context is unfamiliar to us, we’ll utilize weakly informative priors throughout. We’ll balance this prior uncertainty by the `bald_eagles` data in the **bayesrules** package, which includes data on bald eagle sightings during 37 different one-week observation periods. First, get to know this data.\n\na. Construct and discuss a univariate plot of `count`, the number of eagle sightings across the observation periods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbald_eagles <- bald_eagles\n\nbald_eagles %>% \n  ggplot(aes(count_per_week)) +\n  geom_histogram(binwidth = 1, fill = \"#0E0ACC\", color = \"white\", boundary = 0)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nb. Construct and discuss a plot of `count` versus `year.`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(size = 4, color = \"#0E0ACC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nc. In exploring the number of eagle sightings over time, it’s important to consider the fact that the length of the observation periods vary from year to year, ranging from 134 to 248.75 hours. Update your plot from part b to also include information about the observation length in `hours` and comment on your findings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbald_eagles %>% \n  ggplot(aes(year, count)) +\n  geom_smooth(method = \"lm\", color = \"#cc0aa4\", se = FALSE) +\n  geom_smooth(color = \"#7400CC\", se = FALSE) +\n  geom_point(aes(fill = hours), size = 4, shape = 21, color = \"#7400CC\") +\n  scale_x_continuous(breaks = seq(1980,2020,5)) +\n  scale_fill_steps(low = \"white\", high = \"#0E0ACC\") +\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Exercise 12.6 (Eagles: first model attempt) \n\nOur next goal is to model the relationship between bald eagle counts $Y$ by year $X_1$ when controlling for the number of observation hours $X_2$. To begin, consider a Normal regression model of $Y$ versus $X_1$ and $X_2$.\n\na. Simulate the model posterior and check the `prior_summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neagle_model_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = gaussian,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.123 seconds (Warm-up)\nChain 1:                0.139 seconds (Sampling)\nChain 1:                0.262 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.111 seconds (Warm-up)\nChain 2:                0.127 seconds (Sampling)\nChain 2:                0.238 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.114 seconds (Warm-up)\nChain 3:                0.126 seconds (Sampling)\nChain 3:                0.24 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.123 seconds (Warm-up)\nChain 4:                0.114 seconds (Sampling)\nChain 4:                0.237 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nprior_summary(eagle_model_prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'eagle_model_prior' \n------\nIntercept (after predictors centered)\n ~ normal(location = 2, scale = 0.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.70,0.24])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.33)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\n# hi <- bald_eagles %>% \n#   add_epred_draws(eagle_model_prior, ndraws = 50)# %>%\n#   \n# ggplot(hi, aes(year, .epred)) +\n#   geom_line(aes(color = paste(hours, .draw)))\n# \n# ggplot(aes(x = year, y = count, color = hours)) +\n#     geom_line(aes(y = .value, group = paste(hours, .draw))) + \n#   scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n```\n:::\n\n\nb. Use careful notation to write out the complete Bayesian structure of the Normal regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, \\beta_2, \\sigma & \\stackrel{ind}{\\sim} N\\left(\\mu_i, \\sigma^2\\right) \\;\\; \\text{ with } \\;\\; \\mu_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i \\\\\n\\beta_{0c}  & \\sim N\\left(2, .5 \\right)  \\\\\n\\beta_1  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\beta_2  & \\sim N\\left(0, 2.5 \\right) \\\\\n\\sigma   & \\sim \\text{Exp}(1) \\\\\n\\end{aligned}\n\n$$\nc. Complete a `pp_check()` for the Normal model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neagle_model <- update(eagle_model_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.164 seconds (Warm-up)\nChain 1:                0.188 seconds (Sampling)\nChain 1:                0.352 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.18 seconds (Warm-up)\nChain 2:                0.172 seconds (Sampling)\nChain 2:                0.352 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.165 seconds (Warm-up)\nChain 3:                0.202 seconds (Sampling)\nChain 3:                0.367 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.211 seconds (Warm-up)\nChain 4:                0.206 seconds (Sampling)\nChain 4:                0.417 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nmcmc_trace(eagle_model)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(eagle_model)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(eagle_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(1)\npp_check(eagle_model) + \n  xlab(\"count\")\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n:::\n\n\n## Exercise 12.7 (Eagles: second model attempt) \n\nLet’s try to do better. Consider a Poisson regression model of $Y$ versus $X_1$ and $X_2$.\n\na. In the bald eagle analysis, why might a Poisson regression approach be more appropriate than a Normal regression approach?\n\nb. Simulate the posterior of the Poisson regression model of $Y$ versus $X_1$ and $X_2$. Check the `prior_summary()`.\n\n\n::: {.cell hash='12_exercises_cache/html/unnamed-chunk-6_13f89e5f47d0ccd783e22c2fded082a6'}\n\n```{.r .cell-code}\neagle_pois_prior <- stan_glm(count ~ year + hours, \n                             data = bald_eagles, \n                             family = poisson,\n                             prior_intercept = normal(2, 0.5),\n                             prior = normal(0, 2.5, autoscale = TRUE), \n                             chains = 4, iter = 5000*2, seed = 84735, \n                             prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.083 seconds (Warm-up)\nChain 1:                0.118 seconds (Sampling)\nChain 1:                0.201 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.091 seconds (Warm-up)\nChain 2:                0.087 seconds (Sampling)\nChain 2:                0.178 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.076 seconds (Warm-up)\nChain 3:                0.094 seconds (Sampling)\nChain 3:                0.17 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.085 seconds (Warm-up)\nChain 4:                0.093 seconds (Sampling)\nChain 4:                0.178 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nprior_summary(eagle_pois_prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'eagle_pois_prior' \n------\nIntercept (after predictors centered)\n ~ normal(location = 2, scale = 0.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.23,0.08])\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\nbald_eagles %>% \n  add_fitted_draws(eagle_pois_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 400 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nc. Use careful notation to write out the complete Bayesian structure of the Poisson regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nd. Complete a `pp_check()` for the Poisson model. Use this to explain whether the model is “good” and, if not, what assumptions it makes that are inappropriate for the bald eagle analysis.\n\n\n::: {.cell hash='12_exercises_cache/html/unnamed-chunk-7_d98c2b0d3e7dd7e99e40c1ed1fa498f5'}\n\n```{.r .cell-code}\neagle_pois <- update(eagle_pois_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.344 seconds (Warm-up)\nChain 1:                0.362 seconds (Sampling)\nChain 1:                0.706 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.336 seconds (Warm-up)\nChain 2:                0.377 seconds (Sampling)\nChain 2:                0.713 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.349 seconds (Warm-up)\nChain 3:                0.342 seconds (Sampling)\nChain 3:                0.691 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.336 seconds (Warm-up)\nChain 4:                0.403 seconds (Sampling)\nChain 4:                0.739 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nmcmc_trace(eagle_pois)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(eagle_pois)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(eagle_pois)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(1)\npp_check(eagle_pois, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n\n```{.r .cell-code}\npp_check(eagle_pois) + \n  xlab(\"count\")\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-7-5.png){width=672}\n:::\n:::\n\n\n## Exercise 12.8 (Eagles: an even better model) \n\nThe Poisson regression model of bald eagle counts ($Y$) by year ($X_1$) and observation hours ($X_2$), was pretty good. Let’s see if a Negative Binomial approach is even better.\n\na. Simulate the model posterior and use a `pp_check()` to confirm whether the Negative Binomial model is reasonable.\n\n\n::: {.cell hash='12_exercises_cache/html/unnamed-chunk-8_94bd326df9412a6759771219f7b26517'}\n\n```{.r .cell-code}\neagle_negbi_prior <- stan_glm(count ~ year + hours, \n                              data = bald_eagles, \n                              family = neg_binomial_2,\n                              prior_intercept = normal(2, 0.5),\n                              prior = normal(0, 2.5, autoscale = TRUE), \n                              chains = 4, iter = 5000*2, seed = 84735, \n                              prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.133 seconds (Warm-up)\nChain 1:                0.147 seconds (Sampling)\nChain 1:                0.28 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.125 seconds (Warm-up)\nChain 2:                0.115 seconds (Sampling)\nChain 2:                0.24 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.104 seconds (Warm-up)\nChain 3:                0.119 seconds (Sampling)\nChain 3:                0.223 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.115 seconds (Warm-up)\nChain 4:                0.116 seconds (Sampling)\nChain 4:                0.231 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nprior_summary(eagle_negbi_prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'eagle_negbi_prior' \n------\nIntercept (after predictors centered)\n ~ normal(location = 2, scale = 0.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.23,0.08])\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\nbald_eagles %>% \n  add_fitted_draws(eagle_negbi_prior, n = 50) %>%\n  ggplot(aes(x = year, y = count, color = hours)) +\n    geom_line(aes(y = .value, group = paste(hours, .draw))) + \n    ylim(0, 100) +\n  scale_color_steps(low = \"white\", high = \"#0E0ACC\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 382 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\neagle_negbi <- update(eagle_negbi_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.775 seconds (Warm-up)\nChain 1:                0.813 seconds (Sampling)\nChain 1:                1.588 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.842 seconds (Warm-up)\nChain 2:                0.934 seconds (Sampling)\nChain 2:                1.776 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.851 seconds (Warm-up)\nChain 3:                1.12 seconds (Sampling)\nChain 3:                1.971 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.739 seconds (Warm-up)\nChain 4:                0.935 seconds (Sampling)\nChain 4:                1.674 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nmcmc_trace(eagle_negbi)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(eagle_negbi)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(eagle_negbi)\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-4.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(1)\npp_check(eagle_negbi, plotfun = \"hist\", nreps = 5) + \n  xlab(\"count\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n\n```{.r .cell-code}\npp_check(eagle_negbi) + \n  xlab(\"count\")\n```\n\n::: {.cell-output-display}\n![](12_exercises_files/figure-html/unnamed-chunk-8-6.png){width=672}\n:::\n\n```{.r .cell-code}\n# Numerical summaries\ntidy(eagle_negbi, conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term          estimate std.error   conf.low conf.high\n  <chr>            <dbl>     <dbl>      <dbl>     <dbl>\n1 (Intercept) -143.       34.0     -188.       -99.0   \n2 year           0.0715    0.0173     0.0492     0.0945\n3 hours          0.00457   0.00562   -0.00271    0.0120\n```\n:::\n:::\n\n\nb. Use careful notation to write out the complete Bayesian structure of the Negative Binomial regression model of $Y$ by $X_1$ and $X_2$.\n\n$$\n\n$$\n\nc. Interpret the posterior median estimates of the regression coefficients on `year` and `hours`, $\\beta_1$ and $\\beta_2$. Do so on the unlogged scale.\n   - `year` - $+.0715, .0492-.0945$\n   - `hours` - $+.00457, -.00271/.012$\n   \nd. Construct and interpret a 95% posterior credible interval for the `year` coefficient.\n\ne. When controlling for the number of observation hours, do we have ample evidence that the rate of eagle sightings has increased over time?\n\n## Exercise 12.9 (Eagles: model evaluation) \n\nFinally, let’s evaluate the quality of our Negative Binomial bald eagle model.\n\na. How fair is the model?\nb. How wrong is the model?\nc. How accurate are the model predictions?\n\n## Exercise 12.10 (Open exercise: AirBnB) \n\nThe `airbnb_small` data in the **bayesrules** package contains information on AirBnB rentals in Chicago. This data was originally collated by Trinh and Ameri (2016) and distributed by Legler and Roback (2021). In this open-ended exercise, build, interpret, and evaluate a model of the number of `reviews` an AirBnB property has by its `rating`, `district`, `room_type`, and the number of guests it `accommodates`.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}