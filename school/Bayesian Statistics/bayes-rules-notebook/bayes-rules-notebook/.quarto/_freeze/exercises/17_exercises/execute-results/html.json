{
  "hash": "0098811b7d89fd6582c9d392a15bc640",
  "result": {
    "markdown": "---\ntitle: \"17: (Normal) Hierarchical Models with Predictors\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\nlibrary(lme4)\n```\n:::\n\n\n# 17.9.1 Conceptual exercises\n\n## Exercise 17.1 (Translating assumptions into model notation) \n\nTo test the relationship between reaction times and sleep deprivation, researchers enlisted 3 people in a 10-day study. Let $Y_{ij}$ denote the reaction time (in ms) to a given stimulus and $X_{ij}$ the number of days of sleep deprivation for the $i$th observation on subject $j$. For each set of assumptions below, use mathematical notation to represent an appropriate Bayesian hierarchical model of $Y_{ij}$ vs $X_{ij}$.\n\na. Not only do some people tend to react more quickly than others, sleep deprivation might impact some people’s reaction times more than others.\n\n\\begin{aligned}\nY_{ij} | \\beta_{0j}, \\beta_1, \\sigma_y & \\sim N(\\mu_{ij}, \\sigma_y^2) \\;\\; \\text{ with } \\;\\;  \\mu_{ij} = \\beta_{0j} + \\beta_1 X_{ij} \\text{(regression model} \\\\\n& \\text{ WITHIN subject $j$)} \\\\\n\\beta_{0j} | \\beta_0, \\sigma_0 \\stackrel{ind}{\\sim} N(\\beta_0, \\sigma_0^2) & \\text{(variability in baseline speeds} \\\\\n& \\text{ BETWEEN subjects)}\\\\\n\\beta_{0c}  & \\sim N(m_0, s_0^2) \\text{(priors on global parameters)} \\\\\n\\beta_1  & \\sim N(m_1, s_1^2) \\\\\n\\sigma_y & \\sim \\text{Exp}(l_y) \\\\\n\\sigma_0 & \\sim \\text{Exp}(l_0) \\\\\n\\end{aligned}\n\nb. Though some people tend to react more quickly than others, the impact of sleep deprivation on reaction time is the same for all.\nc. Nobody has inherently faster reaction times, though sleep deprivation might impact some people’s reaction times more than others.\n\n## Exercise 17.2 (Sketch the assumption: Part 1) \n\nContinuing with the sleep study, suppose we model the relationship between reaction time $Y_{ij}$ and days of sleep deprivation $X_{ij}$ using the random intercepts model (17.5).\n\na. Sketch a plot of data that we might see if $\\sigma_y > \\sigma_0$.\nb. Explain what $\\sigma_y > \\sigma_0$ would mean in the context of the sleep study.\nc. Repeat part a assuming $\\sigma_y < \\sigma_0$.\nd. Repeat part b assuming $\\sigma_y < \\sigma_0$.\n\n## Exercise 17.3 (Sketch the assumption: Part 2) \n\nSuppose instead that we model the relationship between reaction time $Y_{ij}$ and days of sleep deprivation $X_{ij}$ using the random intercepts and slopes model (17.12) (with different priors).\n\na. Sketch a plot of subject-specific trends that we might see if the correlation parameter were positive $\\rho > 0$.\nb. Explain what $\\rho > 0$ would mean in the context of the sleep study.\nc. Repeat part a for $\\rho < 0$.\nd. Repeat part b for $\\rho < 0$.\n\n## Exercise 17.4 (Making meaning of models) \n\nTo study the relationship between weight and height among pug puppies, you collect data on 10 different litters, each containing 4 to 6 puppies born to the same mother. Let $Y_{ij}$ and $X_{ij}$ denote the weight and height, respectively, for puppy $i$ in litter $j$.\n\na. Write out formal model notation for model 1, a random intercepts model of $Y_{ij}$ vs $X_{ij}$.\nb. Write out formal model notation for model 2, a random intercepts and slopes model of $Y_{ij}$ vs $X_{ij}$.\nc. Summarize the key differences in the assumptions behind models 1 and 2. Root this discussion in the puppy context.\n\n## Exercise 17.5 (Translating models to code) \n\nSuppose we had `weight` and `height` data for the puppy study. Write out appropriate `stan_glmer()` model code for models 1 and 2 from Exercise 17.4.\n\n# 17.9.2 Applied exercises\n\n## Exercise 17.6 (Sleep: setting up the model) \n\nIn the above conceptual exercises, you considered a sleep study of the relationship between reaction time and the number of days of sleep deprivation. You will explore this relationship in more depth here. To this end, suppose researchers tell us that on a typical day, the average person should have a reaction time of roughly 250ms to the stimulus used in the sleep study. Beyond this baseline, we’ll balance weakly informative priors with the `sleepstudy` data from the **lme4** package to better understand reaction times. Specifically, consider two possible models as expressed by `stan_glmer()` syntax:\n\n| model |  formula                             |\n| ----- | ------------------------------------ |\n|  1    | `Reaction ~ Days + (1 | Subject)`    |\n|  2    | `Reaction ~ Days + (Days | Subject)` |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep <- sleepstudy\n\nsleep %>% \n  ggplot(aes(x = Days, y = Reaction, color = Subject)) +\n  geom_smooth(se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\na. What’s the grouping variable in the `sleepstudy` data and why is it important to incorporate this grouping structure into our analysis? **Because each person will have a certain variability among their response times and from the other participants.**\nb. Use formal notation to define the hierarchical regression structure of models 1 and 2. (You will tune the priors in the next exercise.)\n\n\\begin{aligned}\n\\text{Layer 1:} Y_{ij} | \\mu_j, \\sigma_y \\sim &\\text{model of how Reaction varies WITHIN Subject} j \\\\\n\\text{Layer 2:} \\mu_j | \\mu, \\sigma_\\mu \\sim &\\text{model of how the typical Reaction $\\mu_j$ varies BETWEEN Subjects}\\\\\n\\text{Layer 3:} \\mu, \\sigma_y, \\sigma_\\mu \\sim &\\text{prior models for shared global parameters} \\\\\n\\end{aligned}\n\nc. Summarize the key differences between models 1 and 2. Root this discussion in the sleep study.\nd. Using the `sleepstudy` data, construct and discuss a plot that helps you explore which model is more appropriate: 1 or 2.\n\n## Exercise 17.7 (Sleep: simulating the model) \n\nContinuing with the sleep analysis, let’s simulate and dig into the hierarchical posteriors.\n\na. Simulate the posteriors of models 1 and 2. Remember to use a baseline reaction time of 250ms, and weakly informative priors otherwise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep_model_1_prior <- stan_glmer(\n  Reaction ~ Days + (1 | Subject), \n  data = sleep, family = gaussian,\n  prior_intercept = normal(250, 50),\n  prior = normal(2.5, 1), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735, \n  prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.35 seconds (Warm-up)\nChain 1:                0.421 seconds (Sampling)\nChain 1:                0.771 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.344 seconds (Warm-up)\nChain 2:                0.406 seconds (Sampling)\nChain 2:                0.75 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.339 seconds (Warm-up)\nChain 3:                0.306 seconds (Sampling)\nChain 3:                0.645 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.365 seconds (Warm-up)\nChain 4:                0.402 seconds (Sampling)\nChain 4:                0.767 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nset.seed(84735)\nsleep %>% \n  add_fitted_draws(sleep_model_1_prior, n = 4) %>%\n  ggplot(aes(x = Days, y = Reaction)) +\n    geom_line(aes(y = .value, group = paste(Subject, .draw))) + \n    facet_wrap(~ .draw)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsleep %>%\n  add_predicted_draws(sleep_model_1_prior, n = 100) %>%\n  ggplot(aes(x = Reaction)) +\n    geom_density(aes(x = .prediction, group = .draw))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \nIn add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`.\nUse the `ndraws` argument instead.\nSee help(\"tidybayes-deprecated\").\n```\n:::\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(sleep, aes(x = Days, y = Reaction)) + \n  geom_point() + \n  facet_wrap(~ Subject)\n```\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Simulate the posterior\nsleep_model_1 <- update(sleep_model_1_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.56 seconds (Warm-up)\nChain 1:                1.346 seconds (Sampling)\nChain 1:                2.906 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.747 seconds (Warm-up)\nChain 2:                1.588 seconds (Sampling)\nChain 2:                3.335 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.774 seconds (Warm-up)\nChain 3:                1.103 seconds (Sampling)\nChain 3:                2.877 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.718 seconds (Warm-up)\nChain 4:                1.639 seconds (Sampling)\nChain 4:                3.357 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Check the prior specifications\nprior_summary(sleep_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'sleep_model_1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 250, scale = 50)\n\nCoefficients\n ~ normal(location = 2.5, scale = 1)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.018)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\n# Markov chain diagnostics\nmcmc_trace(sleep_model_1)\n```\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(sleep_model_1)\n```\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(sleep_model_1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-6.png){width=672}\n:::\n\n```{.r .cell-code}\nneff_ratio(sleep_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           (Intercept)                                   Days \n                               0.15070                                0.69965 \n            b[(Intercept) Subject:308]             b[(Intercept) Subject:309] \n                               0.26360                                0.29200 \n            b[(Intercept) Subject:310]             b[(Intercept) Subject:330] \n                               0.27535                                0.27240 \n            b[(Intercept) Subject:331]             b[(Intercept) Subject:332] \n                               0.26310                                0.25790 \n            b[(Intercept) Subject:333]             b[(Intercept) Subject:334] \n                               0.26935                                0.26325 \n            b[(Intercept) Subject:335]             b[(Intercept) Subject:337] \n                               0.27675                                0.25160 \n            b[(Intercept) Subject:349]             b[(Intercept) Subject:350] \n                               0.27415                                0.25080 \n            b[(Intercept) Subject:351]             b[(Intercept) Subject:352] \n                               0.25690                                0.26650 \n            b[(Intercept) Subject:369]             b[(Intercept) Subject:370] \n                               0.25060                                0.26010 \n            b[(Intercept) Subject:371]             b[(Intercept) Subject:372] \n                               0.25725                                0.26285 \n                                 sigma Sigma[Subject:(Intercept),(Intercept)] \n                               0.59325                                0.19940 \n```\n:::\n\n```{.r .cell-code}\nrhat(sleep_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           (Intercept)                                   Days \n                              1.001535                               1.000117 \n            b[(Intercept) Subject:308]             b[(Intercept) Subject:309] \n                              1.000673                               1.000071 \n            b[(Intercept) Subject:310]             b[(Intercept) Subject:330] \n                              1.000548                               1.000616 \n            b[(Intercept) Subject:331]             b[(Intercept) Subject:332] \n                              1.000759                               1.000817 \n            b[(Intercept) Subject:333]             b[(Intercept) Subject:334] \n                              1.000902                               1.000580 \n            b[(Intercept) Subject:335]             b[(Intercept) Subject:337] \n                              1.000482                               1.001639 \n            b[(Intercept) Subject:349]             b[(Intercept) Subject:350] \n                              1.000391                               1.000827 \n            b[(Intercept) Subject:351]             b[(Intercept) Subject:352] \n                              1.000844                               1.000702 \n            b[(Intercept) Subject:369]             b[(Intercept) Subject:370] \n                              1.000676                               1.001065 \n            b[(Intercept) Subject:371]             b[(Intercept) Subject:372] \n                              1.000392                               1.000587 \n                                 sigma Sigma[Subject:(Intercept),(Intercept)] \n                              1.000157                               1.000487 \n```\n:::\n\n```{.r .cell-code}\ntidy_summary_1 <- tidy(sleep_model_1, effects = \"fixed\",\n                       conf.int = TRUE, conf.level = 0.80)\ntidy_summary_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   265.       9.48    252.      277.  \n2 Days            7.14     0.687     6.24      8.01\n```\n:::\n\n```{.r .cell-code}\nB0 <- tidy_summary_1$estimate[1]\nB1 <- tidy_summary_1$estimate[2]\nsleep %>%\n  add_fitted_draws(sleep_model_1, n = 200, re_formula = NA) %>%\n  ggplot(aes(x = Days, y = Reaction)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.1) +\n    geom_abline(intercept = B0, slope = B1, color = \"blue\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](17_exercises_files/figure-html/unnamed-chunk-3-7.png){width=672}\n:::\n:::\n\n::: {.cell hash='17_exercises_cache/html/unnamed-chunk-4_4f4ca2d9a15baac96e2614eb616f9c66'}\n\n```{.r .cell-code}\nsleep_model_2 <- stan_glmer(\n  Reaction ~ Days + (Days | Subject), \n  data = sleep, family = gaussian,\n  prior_intercept = normal(250, 50),\n  prior = normal(2.5, 1),\n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735, adapt_delta = 0.99999\n)\n\nsaveRDS(sleep_model_2, \"sleep_model_2.rds\")\n```\n:::\n\n\nb. For model 2, report the global posterior median regression model of `Reaction` time.\nc. For model 2, construct and interpret 80% credible intervals for the `Days` regression coefficient.\nd. For model 2, calculate and interpret the posterior medians of $\\sigma_y$, $\\sigma_0$, $\\sigma_1$, and $\\rho$.\n\n## Exercise 17.8 (Sleep: group-specific inference) \n\nNext, let’s dig into what Model 2 indicates about the individuals that participated in the sleep study.\n\na. Use your posterior simulation to identify the person for whom reaction time changes the least with sleep deprivation. Write out their posterior median regression model.\nb. Repeat part a, this time for the person for whom reaction time changes the most with sleep deprivation.\nc. Use your posterior simulation to identify the person that has the slowest baseline reaction time. Write out their posterior median regression model.\nd. Repeat part c, this time for the person that has the fastest baseline reaction time.\ne. Simulate, plot, and discuss the posterior predictive model of reaction time after 5 days of sleep deprivation for two subjects: you and Subject 308. You’re encouraged to try this from scratch before relying on the `posterior_predict()` shortcut.\n\n## Exercise 17.9 (Sleep: Which model?)\n\na. Evaluate the two models of reaction time: Are they wrong? Are they fair? How accurate are their posterior predictions?\nb. Which of the two models do you prefer and what does this indicate about the relationship between reaction time and sleep deprivation? Justify your answer with posterior evidence.\n\n## Exercise 17.10 (Voices: setting up the model) \n\nDoes one’s voice pitch change depending on attitude? To address this question, Winter and Grawunder (2012) conducted a study in which each subject participated in various role-playing dialogs. These dialogs spanned different contexts (e.g., asking for a favor) and were approached with different attitudes (polite vs informal). In the next exercises you’ll explore a hierarchical regression analysis of $Y_{ij}$, the average voice pitch in subject $j$’s $i$th dialog session (measured in Hz), by $X_{ij}$, whether or not the dialog was polite (vs informal). Beyond a baseline understanding that the typical voice pitch is around 200 Hz, you should utilize weakly informative priors.\n\na. Using formal notation, define the hierarchical regression model of $Y_{ij}$ vs $X_{ij1}$. In doing so, assume that baseline voice pitch differs from subject to subject, but that the impact of attitude on voice pitch is similar among all subjects.\nb. Compare and contrast the meanings of model parameters $\\beta_{0j}$ and $\\beta_0$ in the context of this voice pitch study. NOTE: Remember that $X_{ij1}$ is a categorical indicator variable.\nc. Compare and contrast the meanings of model parameters $\\sigma_y$ and $\\sigma_0$ in the context of this voice pitch study.\n\n## Exercise 17.11 (Voices: check out some data) \n\nTo balance our weakly informative priors for the model of pitch by attitude, check out some data.\n\na. Load the voices data from the bayesrules package. How many study subjects are included in this sample? In how many dialogs did each subject participate?\nb. Construct and discuss a plot which illustrates the relationship between voice pitch and attitude both within and between subjects.\n\n## Exercise 17.12 (Voices: simulating the model) \n\nContinuing with the voice pitch analysis, in this exercise you will simulate and dig into the hierarchical posterior of your model parameters.\n\na. Simulate the hierarchical posterior model of voice pitch by attitude. Construct and discuss trace plots, density plots, autocorrelation plots, and a pp_check() of the chain output.\nb. Construct and interpret a 95% credible interval for $\\beta_0$.\nc. Construct and interpret a 95% credible interval for $\\beta_1$.\nd. Is there ample evidence that, for the average subject, voice pitch differs depending on attitude (polite vs informal)? Explain.\n\n## Exercise 17.13 (Voices: focusing on the individual) \n\nContinuing with the voice pitch analysis, in this exercise you will focus on specific subjects.\n\na. Report the global posterior median model of the relationship between voice pitch and attitude.\nb. Report and contrast the posterior median models for two subjects in our data: A and F.\nc. Using `posterior_predict()`, simulate posterior predictive models of voice pitch in a new polite dialog for three different subjects: A, F, and you. Illustrate your simulation results using `mcmc_areas()` and discuss your findings.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}