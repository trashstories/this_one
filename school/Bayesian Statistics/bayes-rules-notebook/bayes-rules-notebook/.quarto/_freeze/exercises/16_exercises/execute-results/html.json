{
  "hash": "3cca4d0cb16353ea352e8f02a6bb70be",
  "result": {
    "markdown": "---\ntitle: \"16: (Normal) Hierarchical Models without Predictors\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\nlibrary(forcats)\nlibrary(patchwork)\nlibrary(ggridges)\n\ndata(climbers_sub)\ndata(coffee_ratings)\n```\n:::\n\n\n## Exercise 16.2 (Grouping variable or predictor?)\n\na. The `climbers_sub` data in the `bayesrules` package contains outcomes for 2076 climbers that have sought to summit a Himalayan mountain peak. In a model of climber `success`, is `expedition_id` a potential predictor or a grouping variable? Explain.\n   - **Group because there are many climbers for each `expedition_id`**\nb. In a model of climber `success`, is `season` a potential predictor or a grouping variable? Explain.\n   - **Potential predictor because it may influence success for individual climbers and the expedition as a group**\nc. The `coffee_ratings` data in the `bayesrules` package contains ratings for 1339 different coffee batches. In a model of coffee ratings (`total_cup_points`), is `processing_method` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Group because there are multiple methods for each `farm_name`, or maybe not heirarchical with thses variables**\nd. In a model of coffee ratings (`total_cup_points`), is `farm_name` a potential predictor or a grouping variable? Explain.\n   - ***Maybe* Potential predictor because it may influence cup points for individual farms, or maybe not heirarchical with thses variables**\n   \n## Exercise 16.3 (Speed typing: interpret the coefficients) \n\nAlicia loves typing. To share the appreciation, she invites four friends to each take 20 speed-typing tests. Let $Y_{ij}$ be the time it takes friend $j$ to complete test $i$.\n\na. In modeling $Y_{ij}$, explain why it’s important to account for the grouping structure introduced by observing each friend multiple times.\nb. Suppose we were to model the outcomes $Y_{ij}$ by (16.5). Interpret the meaning of all model coefficients in terms of what they might illuminate about typing speeds: $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$.\n   - $\\mu_j =$ mean typing time per person\n   - $\\mu =$ global priors\n   - $\\sigma_y =$ variation within each friend\n   - $\\sigma_\\mu = $ variation between friends\n   \n## Exercise 16.4 (Speed typing: sketch the data) \n\nIn the spirit of Figure 16.8, sketch what density plots of your four friends’ typing speed outcomes $Y_{ij}$ might look like under each scenario below.\n\na. The overall results of the 20 timed tests are remarkably similar among the four friends.\nb. Each person is quite consistent in their typing times, but there are big differences from person to person – some tend to type much faster than others.\nc. Within the subjects, there doesn’t appear to be much correlation in typing time from test to test.\n\n# 16.9.2 Applied exercises\n\n## Exercise 16.6 (Big words: getting to know the data) \n\nRecall from Section 16.7 the Abdul Latif Jameel Poverty Action Lab (J-PAL) study into the effectiveness of a digital vocabulary learning program, the Big Word Club (BWC) (Kalil, Mayer, and Oreopoulos 2020). In our analysis of this program, we’ll utilize weakly informative priors with a baseline understanding that the average student saw 0 change in their vocabulary scores throughout the program. We’ll balance these priors by the `big_word_club` data in the **bayesrules** package. For each student participant, `big_word_club` includes a `school_id` and the *percentage change* in vocabulary scores over the course of the study period (`score_pct_change`). We keep only the students that participated in the BWC program (`treat == 1`), and thus eliminate the control group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"big_word_club\")\nbig_word_club <- big_word_club %>% \n  filter(treat == 1) %>% \n  select(school_id, score_pct_change) %>% \n  na.omit()\n```\n:::\n\n\na. How many schools participated in the Big Word Club? **26**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(mean = mean(score_pct_change))\n```\n:::\n\n\nb. What’s the range in the number of student participants per school? **12-17**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum2 <- big_word_club %>% \n  group_by(school_id) %>% \n  summarise(n = n())\n\nmin(sum2$n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12\n```\n:::\n\n```{.r .cell-code}\nmax(sum2$n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17\n```\n:::\n:::\n\n\nc. On average, at which school did students exhibit the greatest improvement in vocabulary? The least? **17**\n\nd. Construct and discuss a plot which illustrates the variability in `score_pct_change` within and between schools. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- big_word_club %>% \n  ggplot(aes(score_pct_change, fill = school_id)) +\n  geom_density(alpha = .5, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .5) +\n  theme_light()\n\nw / b\n```\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nw2 <- big_word_club %>% \n  ggplot(aes(score_pct_change, school_id, fill = school_id)) +\n  geom_density_ridges(alpha = .75, color = \"white\") +\n  theme_light() +\n  theme(legend.position = \"none\")\n\nb2 <- sum %>% \n  ggplot(aes(mean)) +\n  geom_density(fill = \"#7400CC\", color = \"white\", alpha = .75) +\n  theme_light()\n\nw2 / b2 +\n  plot_layout(heights = c(80, 20))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPicking joint bandwidth of 7.17\n```\n:::\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n## Exercise 16.7 (Big words: setting up the model) \n\nIn the next exercises you will explore a hierarchical one-way ANOVA model (16.12) of $Y_{ij}$, the percentage change in vocabulary scores, for student $i$ in school $j$.\n\na. Why is a hierarchical model, vs a complete or no pooled model, appropriate in our analysis of the BWC program?\n   - **To account for variation within and between schools**\nb. Compare and contrast the meanings of model parameters $\\mu$ and $\\mu_j$ in the context of this vocabulary study.\n   - $\\mu_j =$ mean score change time per school\n   - $\\mu =$ global priors\nc. Compare and contrast the meanings of model parameters $\\sigma_y$ and $\\sigma_\\mu$ in the context of this vocabulary study.\n   - $\\sigma_y =$ variation within each school\n   - $\\sigma_\\mu = $ variation between schools\n\n## Exercise 16.8 (Big words: simulating the model)\n\na. Simulate the hierarchical posterior model of parameters $(\\mu_j,\\mu,\\sigma_y,\\sigma_\\mu)$ using 4 chains, each of length 10000.\n\n\n::: {.cell hash='16_exercises_cache/html/unnamed-chunk-6_30570c4a9463e7c6e5d243e7cdb9f22f'}\n\n```{.r .cell-code}\nbig_hier <- stan_glmer(\n  score_pct_change ~ (1 | school_id), \n  data = big_word_club , family = gaussian,\n  prior_intercept = normal(5, 2.5, autoscale = TRUE),\n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = .1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.809 seconds (Warm-up)\nChain 1:                1.958 seconds (Sampling)\nChain 1:                3.767 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.562 seconds (Warm-up)\nChain 2:                1.175 seconds (Sampling)\nChain 2:                2.737 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.574 seconds (Warm-up)\nChain 3:                1.767 seconds (Sampling)\nChain 3:                3.341 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.421 seconds (Warm-up)\nChain 4:                1.363 seconds (Sampling)\nChain 4:                2.784 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nb. Construct and discuss Markov chain trace, density, and autocorrelation plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confirm the prior tunings\nprior_summary(big_hier)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'big_hier' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 5, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 5, scale = 43)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.059)\n\nCovariance\n ~ decov(reg. = 0.1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\nmcmc_trace(big_hier)\n```\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(big_hier)\n```\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(big_hier)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\nneff_ratio(big_hier)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             (Intercept) \n                                 0.81685 \n              b[(Intercept) school_id:2] \n                                 0.62440 \n              b[(Intercept) school_id:3] \n                                 0.62120 \n              b[(Intercept) school_id:4] \n                                 1.12315 \n              b[(Intercept) school_id:6] \n                                 0.64515 \n              b[(Intercept) school_id:7] \n                                 1.15820 \n              b[(Intercept) school_id:8] \n                                 0.71110 \n              b[(Intercept) school_id:9] \n                                 0.64160 \n             b[(Intercept) school_id:10] \n                                 1.01465 \n             b[(Intercept) school_id:11] \n                                 1.22280 \n             b[(Intercept) school_id:14] \n                                 0.65555 \n             b[(Intercept) school_id:16] \n                                 1.19760 \n             b[(Intercept) school_id:17] \n                                 0.59945 \n             b[(Intercept) school_id:18] \n                                 0.87910 \n             b[(Intercept) school_id:21] \n                                 1.06470 \n             b[(Intercept) school_id:22] \n                                 1.16215 \n             b[(Intercept) school_id:25] \n                                 1.21655 \n             b[(Intercept) school_id:28] \n                                 0.96495 \n             b[(Intercept) school_id:29] \n                                 1.22290 \n             b[(Intercept) school_id:32] \n                                 1.13440 \n             b[(Intercept) school_id:34] \n                                 0.66215 \n             b[(Intercept) school_id:37] \n                                 0.87485 \n             b[(Intercept) school_id:38] \n                                 1.15915 \n             b[(Intercept) school_id:39] \n                                 1.13995 \n             b[(Intercept) school_id:41] \n                                 0.77645 \n             b[(Intercept) school_id:43] \n                                 1.06040 \n             b[(Intercept) school_id:44] \n                                 0.98325 \n                                   sigma \n                                 1.00960 \nSigma[school_id:(Intercept),(Intercept)] \n                                 0.36390 \n```\n:::\n\n```{.r .cell-code}\nrhat(big_hier)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             (Intercept) \n                               1.0000143 \n              b[(Intercept) school_id:2] \n                               1.0000228 \n              b[(Intercept) school_id:3] \n                               0.9999558 \n              b[(Intercept) school_id:4] \n                               0.9999602 \n              b[(Intercept) school_id:6] \n                               1.0002087 \n              b[(Intercept) school_id:7] \n                               0.9999820 \n              b[(Intercept) school_id:8] \n                               1.0000057 \n              b[(Intercept) school_id:9] \n                               1.0003271 \n             b[(Intercept) school_id:10] \n                               1.0003279 \n             b[(Intercept) school_id:11] \n                               1.0000093 \n             b[(Intercept) school_id:14] \n                               1.0000920 \n             b[(Intercept) school_id:16] \n                               0.9999496 \n             b[(Intercept) school_id:17] \n                               1.0002641 \n             b[(Intercept) school_id:18] \n                               0.9998919 \n             b[(Intercept) school_id:21] \n                               1.0000552 \n             b[(Intercept) school_id:22] \n                               0.9999522 \n             b[(Intercept) school_id:25] \n                               0.9999250 \n             b[(Intercept) school_id:28] \n                               0.9999944 \n             b[(Intercept) school_id:29] \n                               0.9998924 \n             b[(Intercept) school_id:32] \n                               0.9998532 \n             b[(Intercept) school_id:34] \n                               1.0003213 \n             b[(Intercept) school_id:37] \n                               0.9999625 \n             b[(Intercept) school_id:38] \n                               0.9999169 \n             b[(Intercept) school_id:39] \n                               0.9998560 \n             b[(Intercept) school_id:41] \n                               1.0000822 \n             b[(Intercept) school_id:43] \n                               1.0000498 \n             b[(Intercept) school_id:44] \n                               1.0000443 \n                                   sigma \n                               1.0001453 \nSigma[school_id:(Intercept),(Intercept)] \n                               1.0007189 \n```\n:::\n:::\n\n\nc. Construct and discuss a `pp_check()` of the chain output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(big_hier) + \n  xlab(\"score change\")\n```\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Store the simulation in a data frame\nbig_hier_df <- as.data.frame(big_hier)\n\n# Check out the first 3 and last 3 parameter labels\nbig_hier_df %>% \n  colnames() %>% \n  as.data.frame() %>% \n  slice(1:3, 13:14)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            .\n1                 (Intercept)\n2  b[(Intercept) school_id:2]\n3  b[(Intercept) school_id:3]\n4 b[(Intercept) school_id:17]\n5 b[(Intercept) school_id:18]\n```\n:::\n:::\n\n\n## Exercise 16.9 (Big words: global parameters) \n\nIn this exercise, we’ll explore the global parameters of our BWC model: $(\\mu,\\sigma_y,\\sigma_\\mu)$.\n\na. Construct and interpret a 95% credible interval for $\\mu$.\n   - **There's a 95% chance that the average school saw an increase between 4.19 and 8.44.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(big_hier, effects = \"fixed\", \n     conf.int = TRUE, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)     6.29      1.06     4.19      8.44\n```\n:::\n:::\n\n\nb. Is there ample evidence that, on average, student vocabulary levels improve throughout the vocabulary program? Explain.\n   - **There is ample evidence that, on average, student vocabulary levels improved throughout the vocabulary program by at least 4.19. According to the global parameters of the model 95% of the simulated outcomes were between 4.19 and 8.44.**\nc. Which appears to be larger: the variability in vocabulary performance *between* or *within* schools? Provide posterior evidence and explain the implication of this result in the context of the analysis.\n   - **The variation within schools (16.9) was much higher than the variation between schools (2.88).**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(big_hier, effects = \"ran_pars\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  term                     group     estimate\n  <chr>                    <chr>        <dbl>\n1 sd_(Intercept).school_id school_id     2.88\n2 sd_Observation.Residual  Residual     16.9 \n```\n:::\n:::\n\n\n## Exercise 16.10 (Big words: focusing on schools) \n\nNext, let’s dig into the school-specific means, $\\mu_j$.\n\na. Construct and discuss a plot of the 80% posterior credible intervals for the average percent change in vocabulary score for all schools in the study, $\\mu_j$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool_sum <- tidy(big_hier, effects = \"ran_vals\", \n                   conf.int = TRUE, conf.level = 0.80)\n\n# Check out the results for the first & last 2 artists\nschool_sum %>% \n  select(level, conf.low, conf.high) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 26 × 3\n   level conf.low conf.high\n   <chr>    <dbl>     <dbl>\n 1 2       -0.698     6.04 \n 2 3       -0.696     5.74 \n 3 4       -1.98      3.41 \n 4 6       -5.48      0.832\n 5 7       -3.66      2.00 \n 6 8       -1.10      4.97 \n 7 9       -0.874     5.36 \n 8 10      -3.92      1.54 \n 9 11      -2.29      3.15 \n10 14      -5.38      0.881\n# ℹ 16 more rows\n```\n:::\n\n```{.r .cell-code}\n# Get MCMC chains for each mu_j\nschool_chains <- big_hier %>%\n  spread_draws(`(Intercept)`, b[,school_id]) %>% \n  mutate(mu_j = `(Intercept)` + b) \n\n# Check it out\nschool_chains %>% \n  select(school_id, `(Intercept)`, b, mu_j) %>% \n  head(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n# Groups:   school_id [4]\n  school_id    `(Intercept)`      b  mu_j\n  <chr>                <dbl>  <dbl> <dbl>\n1 school_id:10          7.28 -0.532  6.75\n2 school_id:11          7.28  1.59   8.87\n3 school_id:14          7.28 -1.89   5.39\n4 school_id:16          7.28 -0.209  7.07\n```\n:::\n\n```{.r .cell-code}\n# Get posterior summaries for mu_j\nschool_sum_scaled <- school_chains %>% \n  select(-`(Intercept)`, -b) %>% \n  mean_qi(.width = 0.80) %>% \n  mutate(school_id = fct_reorder(school_id, mu_j))\n\n# Check out the results\nschool_sum_scaled %>% \n  select(school_id, mu_j, .lower, .upper) %>% \n  head(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  school_id     mu_j .lower .upper\n  <fct>        <dbl>  <dbl>  <dbl>\n1 school_id:10  5.42  2.36    8.10\n2 school_id:11  6.65  3.85    9.61\n3 school_id:14  4.55  0.892   7.52\n4 school_id:16  6.22  3.28    9.13\n```\n:::\n\n```{.r .cell-code}\nggplot(school_sum_scaled, \n       aes(school_id, mu_j, ymin = .lower, ymax = .upper)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xaxis_text(angle = 90, hjust = 1) +\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nb. Construct and interpret the 80% posterior credible interval for $\\mu_10$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum %>% \n  filter(school_id %in% c(\"10\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  school_id  mean\n  <fct>     <dbl>\n1 10         3.04\n```\n:::\n\n```{.r .cell-code}\n# Simulate School 10's posterior predictive model\nset.seed(84735)\nchains10 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:10]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_10 = `(Intercept)` + b,\n         y_10 = rnorm(20000, mean = mu_10, sd = sigma))\n\n# Check it out\nhead(chains10, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)          b    sigma     mu_10      y_10\n1    7.280455 -0.5319727 16.31960 6.7484823 17.637450\n2    6.250317 -0.8239229 18.29528 5.4263945  3.160702\n3    5.210317 -4.3548292 16.96257 0.8554883 14.101007\n```\n:::\n\n```{.r .cell-code}\n# Posterior summary of Y_new,j\nchains10 %>% \n  mean_qi(y_10, .width = 0.80)    \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n   y_10 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  5.39  -16.3   27.4    0.8 mean   qi       \n```\n:::\n\n```{.r .cell-code}\n# Posterior summary of mu_j\nschool_sum_scaled %>% \n  filter(school_id == \"school_id:10\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 7\n  school_id     mu_j .lower .upper .width .point .interval\n  <fct>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 school_id:10  5.42   2.36   8.10    0.8 mean   qi       \n```\n:::\n:::\n\n\nc. Is there ample evidence that, on average, vocabulary scores at School 10 improved by more than 5% throughout the duration of the vocabulary program? Provide posterior evidence.\n   - **Yes because the lower confidence level is 2.36 for school 10.**\n\n## Exercise 16.11 (Big words: predicting vocab levels) \n\nSuppose we *continue* the vocabulary study at each of Schools 6 and 17 (which participated in the current study) and Bayes Prep, a school which is new to the study. In this exercise you’ll make predictions about $Y_{\\text{new},j}$, the vocabulary performance of a student that’s new to the study from each of these three schools $j$.\n\na. *Without* using the `posterior_predict()` shortcut function, simulate posterior predictive models of $Y_{\\text{new},j}$ for School 6 and Bayes Prep. Display the first 6 posterior predictions for both schools.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\nbp_chains <- big_hier_df %>%\n  mutate(sigma_mu = sqrt(`Sigma[school_id:(Intercept),(Intercept)]`),\n         mu_bp = rnorm(20000, `(Intercept)`, sigma_mu),\n         y_bp = rnorm(20000, mu_bp, sigma))\n\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n   y_bp .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  6.10  -15.8   28.3    0.8 mean   qi       \n```\n:::\n\n```{.r .cell-code}\n# Simulate School 6's posterior predictive model\nset.seed(84735)\nchains6 <- big_hier_df %>%\n  rename(b = `b[(Intercept) school_id:6]`) %>% \n  select(`(Intercept)`, b, sigma) %>% \n  mutate(mu_6 = `(Intercept)` + b,\n         y_6 = rnorm(20000, mean = mu_6, sd = sigma))\n\n# Posterior summary of Y_new,j\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)    \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n    y_6 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  4.46  -17.3   26.6    0.8 mean   qi       \n```\n:::\n\n```{.r .cell-code}\nhead(bp_chains, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) b[(Intercept) school_id:2] b[(Intercept) school_id:3]\n1    7.280455                  1.2389285                 -0.9198388\n2    6.250317                 -0.3540212                  1.4745700\n3    5.210317                  5.3583983                  1.1139317\n4    5.814160                  3.9145483                  7.9402123\n5    7.683257                 -2.6919321                 -2.5195240\n6    4.964843                 14.7946997                 12.0392847\n  b[(Intercept) school_id:4] b[(Intercept) school_id:6]\n1                   2.034426                 -1.3227025\n2                  -1.210195                 -0.3121820\n3                   3.832051                 -4.5237832\n4                  -0.886879                 -0.1299664\n5                  -1.031272                 -1.8837906\n6                   3.220847                 -2.8513144\n  b[(Intercept) school_id:7] b[(Intercept) school_id:8]\n1                  0.2332278                 -0.8130400\n2                 -0.6210570                  1.1337475\n3                 -3.4405832                  5.1692990\n4                  2.6848368                 -0.5986590\n5                  0.5201174                  1.7324784\n6                 -1.3618097                  0.3260983\n  b[(Intercept) school_id:9] b[(Intercept) school_id:10]\n1                 0.06915649                  -0.5319727\n2                 0.95884114                  -0.8239229\n3                 4.10042371                  -4.3548292\n4                 1.56441200                   2.8584343\n5                 0.58065560                  -3.7303526\n6                 5.63419404                  -0.3761934\n  b[(Intercept) school_id:11] b[(Intercept) school_id:14]\n1                    1.592754                  -1.8899547\n2                   -1.335669                   1.3966916\n3                    4.085472                  -2.3392986\n4                   -1.048096                  -5.1580987\n5                    1.010326                  -0.6915087\n6                    3.481440                  -7.4536192\n  b[(Intercept) school_id:16] b[(Intercept) school_id:17]\n1                  -0.2085689                   -1.526679\n2                  -0.6845183                    2.425648\n3                   2.4025841                    6.540442\n4                  -0.2657918                    2.634860\n5                  -2.1582579                    3.463109\n6                  -0.9751888                   -1.344781\n  b[(Intercept) school_id:18] b[(Intercept) school_id:21]\n1                   -1.054147                   0.3753659\n2                    1.371660                  -1.7998270\n3                    1.525159                  -1.7439501\n4                    5.915941                   1.2035745\n5                   -1.442599                  -0.5351036\n6                    7.745026                  -4.0566444\n  b[(Intercept) school_id:22] b[(Intercept) school_id:25]\n1                  -1.0049721                  -0.2405114\n2                  -0.8317382                   0.6124255\n3                  -1.8019600                   3.2335822\n4                   3.5648380                  -5.8654467\n5                  -5.2459998                   1.1345373\n6                   5.4832884                  -2.2190573\n  b[(Intercept) school_id:28] b[(Intercept) school_id:29]\n1                  -0.7923043                   0.8484584\n2                  -0.4485169                  -1.8478271\n3                   2.8095975                   1.2830770\n4                  -0.5748321                  -2.3061004\n5                   0.1250219                  -1.1519059\n6                   2.9740900                   5.0160545\n  b[(Intercept) school_id:32] b[(Intercept) school_id:34]\n1                  -1.3171823                  -0.7199433\n2                  -0.1400575                   0.1059746\n3                  -0.6018561                  -1.1344749\n4                  -0.2281797                  -3.2519650\n5                  -2.4272615                  -2.9035388\n6                   4.4481856                   1.1727995\n  b[(Intercept) school_id:37] b[(Intercept) school_id:38]\n1                  -1.4330409                  -0.6838630\n2                   1.0362183                  -0.3562393\n3                  -0.3830766                  -1.9215252\n4                  -4.5257394                   6.2814330\n5                  -1.6404081                  -0.8879844\n6                  -3.1296066                   2.8681968\n  b[(Intercept) school_id:39] b[(Intercept) school_id:41]\n1                   1.0905825                  -0.5237865\n2                  -0.6246341                  -0.6788548\n3                   1.4105439                   1.4662792\n4                   2.3695029                  -9.4110188\n5                   1.2194303                   3.2528417\n6                  -1.3815802                  -8.5751787\n  b[(Intercept) school_id:43] b[(Intercept) school_id:44]    sigma\n1                   -2.351573                   2.1361801 16.31960\n2                    1.712156                  -2.6046824 18.29528\n3                    3.077509                   1.4545614 16.96257\n4                   -5.898572                  -3.4456674 15.57253\n5                    1.745279                   0.5549951 17.73611\n6                   -7.053012                  -6.1702074 16.19826\n  Sigma[school_id:(Intercept),(Intercept)] sigma_mu     mu_bp        y_bp\n1                                 1.381416 1.175337  8.064678   5.1350107\n2                                 1.402940 1.184458  6.103634 -15.5738279\n3                                16.770450 4.095174  8.408106  15.3628347\n4                                18.550405 4.307018  6.834571   0.5334868\n5                                 6.765105 2.600982  9.754967 -11.1361355\n6                                40.869480 6.392924 15.086425  44.6239998\n```\n:::\n\n```{.r .cell-code}\nhead(chains6, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)          b    sigma      mu_6       y_6\n1    7.280455 -1.3227025 16.31960 5.9577524 16.846720\n2    6.250317 -0.3121820 18.29528 5.9381354  3.672443\n3    5.210317 -4.5237832 16.96257 0.6865343 13.932054\n4    5.814160 -0.1299664 15.57253 5.6841941  9.373609\n5    7.683257 -1.8837906 17.73611 5.7994662 19.926471\n6    4.964843 -2.8513144 16.19826 2.1135288 27.759382\n```\n:::\n:::\n\n\nb. Using your simulations from part (a), construct, interpret, and compare the 80% posterior predictive intervals of $Y_{\\text{new},j}$ for School 6 and Bayes Prep.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior predictive summaries\nbp_chains %>% \n  mean_qi(y_bp, .width = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n   y_bp .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  6.10  -15.8   28.3    0.8 mean   qi       \n```\n:::\n\n```{.r .cell-code}\nchains6 %>% \n  mean_qi(y_6, .width = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n    y_6 .lower .upper .width .point .interval\n  <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  4.46  -17.3   26.6    0.8 mean   qi       \n```\n:::\n:::\n\n\nc. Using `posterior_predict()` this time, simulate posterior predictive models of $Y_{\\text{new},j}$ for each of School 6, School 17, and Bayes Prep. Illustrate your simulation results using `mcmc_areas()` and discuss your findings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\nprediction_shortcut <- posterior_predict(\n  big_hier,\n  newdata = data.frame(school_id = c(\"6\", \"10\", \"BP\")))\n```\n:::\n\n\nd. Finally, construct, plot, and discuss the 80% posterior prediction intervals for all schools in the original study.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior predictive model plots\nmcmc_areas(prediction_shortcut, prob = 0.8) +\n  scale_y_discrete(labels = c(\"6\", \"10\", \"BP\")) +\n  geom_vline(xintercept = 0, color = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n```\n:::\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Exercise 16.12 (Big words: shrinkage) \n\nRe-examine the posterior predictive plots from Exercise 16.11. Would you say that there is a little or a lot of shrinkage in this analysis? Provide evidence and explain why you think this is the case.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\npredictions_hierarchical <- posterior_predict(big_hier, \n                                              newdata = sum)\n\n# Posterior predictive plots\nppc_intervals(sum$mean, yrep = predictions_hierarchical, \n              prob_outer = 0.80) +\n  scale_x_continuous(labels = sum$school_id, \n                     breaks = 1:nrow(sum)) +\n  xaxis_text(angle = 90, hjust = 1) + \n  geom_hline(yintercept = mean(big_word_club))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in mean.default(big_word_club): argument is not numeric or logical:\nreturning NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (`geom_hline()`).\n```\n:::\n\n::: {.cell-output-display}\n![](16_exercises_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}