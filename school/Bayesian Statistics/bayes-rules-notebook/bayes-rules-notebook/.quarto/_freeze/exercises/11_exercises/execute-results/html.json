{
  "hash": "83d3256fd53c03d212652c4f4e4f6e4f",
  "result": {
    "markdown": "---\ntitle: \"11: Extending the Normal Regression Model\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load some packages\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(tidybayes)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \n            \"#CC5514\", \"#0ACCC5\")\n```\n:::\n\n\n## Exercise 11.1 (Why multiple predictors?) \n\nBriefly explain why we might want to build a regression model with more than one predictor.\n\n## Exercise 11.2 (Categorical predictors: cars) \n\nLet’s say that you want to model a car’s miles per gallon in a city ($Y$) by the make of the car: Ford, Kia, Subaru, or Toyota. This relationship can be written as $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+\\beta_3X_{3}$, where $X_1, X_2, X_3$ are indicators for whether or not the cars are Kias, Subarus, or Toyotas, respectively:\n\na. Explain why there is no indicator term for the Ford category. \n   - **Ford is the reference group**\nb. Interpret the regression coefficient $\\beta_2$. \n   - **The expected change in MPG if the car is a Subaru**\nc. Interpret the regression coefficient $\\beta_0$. \n   - **The mean MPG if the car is a Ford**\n\n## Exercise 11.3 (Categorical and quantitative predictors: tomatoes) \n\nYou have recently taken up the hobby of growing tomatoes and hope to learn more about the factors associated with bigger tomatoes. As such, you plan to model a tomato’s weight in grams ($Y$) by the number of days it has been growing ($X_1$) and its type, Mr. Stripey or Roma. Suppose the expected weight of a tomato is a linear function of its age and type, $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}$, where $X_2$ is an indicator for Roma tomatoes.\n\na. Interpret each regression coefficient, $\\beta_0$, $\\beta_1$, and $\\beta_2$.\n   i. **$\\beta_0 =$ y-intercept (expected mean of Mr. Stripey tomatoes and zero days)** \n   ii. **$\\beta_1 =$ expected change for Mr. Stripey tomatoes with each additional day**\n   iii. **$\\beta_2 =$ expected change if Roma tomatoes if days are held constant**\nb. What would it mean if $\\beta_2$ were equal to zero?\n   - **Mr. Stripey tomatoes (reference group)**\n\n## Exercise 11.4 (Interactions: tomatoes) \n\nContinuing your quest to understand tomato size, you incorporate an interaction term between the tomato grow time ($X_1$) and type ($X_2$) into your model: $\\mu = \\beta_0 + \\beta_1X_{1} + \\beta_2X_{2}+ \\beta_3X_{1}X_{2}$.\n\na. Explain, in context, what it means for $X_1$ and $X_2$ to interact.\n   - **That there is an added effect when $X_1$ and $X_2$ both change in addition to the effect of only one changing**\nb. Interpret $\\beta_3$.\n   - **$\\beta_3 =$ the additional expected change when tomatoes are Roma and increase by one day**\n\n## Exercise 11.5 (Interaction terms)\n\na. Sketch a model that would benefit from including an interaction term between a categorical and quantitative predictor.\nb. Sketch a model that would not benefit from including an interaction term between a categorical and quantitative predictor.\nc. Besides visualization, what are two other ways to determine if you should include interaction terms in your model?\n\n## Exercise 11.6 (Improving your model: shoe size) \n\nLet’s say you model a child’s shoe size ($Y$) by two predictors: the child’s age in years ($X_1$) and an indicator of whether the child knows how to swim ($X_2$).\n\na. Generally speaking, why can it be beneficial to add predictors to models?\n   - **To achieve a better comparison across similar groups**\nb. Generally speaking, why can it be beneficial to remove predictors from models?\n   - **Too many predictors can hide the strength of correlations by attributing it to another (i.e. higher age = more likely to swim, making swimming look like a predictor but truly it is age)**\nc. What might you *add* to this model to improve your predictions of shoe size? Why?\n   - **Gender, since boys and girls may have different average show size by gender**\nd. What might you *remove* from this model to improve it? Why?\n   - **Ability to swim since age probably does a better job of predicting shoe size than ability to swim**\n\n## Exercise 11.7 (What makes a good model?) \n\nWe don’t expect our regression models to be perfect, but we do want to do our best. It can be helpful to think about what we want and expect from our models.\n\na. What are qualities of a good model?\n   - **Enough predictors to explain any confounding factors**\nb. What are qualities of a bad model?\n   - **Too many or too few predictors that over or under inflate correlations**\n\n## Exercise 11.8 (Is our model good / better?) \n\nWhat techniques have you learned in this chapter to assess and compare your models? Give a brief explanation for each technique.\n\n## Exercise 11.9 (Bias-variance trade-off) \n\nIn your own words, briefly explain what the bias-variance tradeoff is and why it is important.\n\n\n# 11.7.2 Applied exercises\n\nIn the next exercises you will use the `penguins_bayes` data in the **bayesrules** package to build various models of penguin `body_mass_g.` Throughout, we’ll utilize weakly informative priors and a basic understanding that the average penguin weighs somewhere between 3,500 and 4,500 grams. Further, one predictor of interest is penguin `species`: Adelie, Chinstrap, or Gentoo. We’ll get our first experience with a 3-level predictor like this in Chapter 12. If you’d like to work with only 2 levels as you did in Chapter 11, you can utilize the `penguin_data` which includes only Adelie and Gentoo penguins:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Alternative penguin data\npenguin_data <- penguins_bayes %>% \n  filter(species %in% c(\"Adelie\", \"Gentoo\"))\n```\n:::\n\n\n## Exercise 11.10 (Penguins! Main effects) \n\nLet’s begin our analysis of penguin `body_mass_g` by exploring its relationship with `flipper_length_mm` and `species`.\n\na. Plot and summarize the observed relationships among these three variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_data %>% \n  ggplot(aes(body_mass_g, flipper_length_mm, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(values = colors) +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\npenguin_data %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors) +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsum <- penguin_data %>% \n  drop_na(body_mass_g, flipper_length_mm) %>% \n  group_by(species) %>% \n  summarise(bm = mean(body_mass_g),\n            fl = mean(flipper_length_mm))\n\nsum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  species    bm    fl\n  <fct>   <dbl> <dbl>\n1 Adelie  3701.  190.\n2 Gentoo  5076.  217.\n```\n:::\n:::\n\n\nb. Use `stan_glm()` to simulate a posterior Normal regression model of `body_mass_g` by `flipper_length_mm` and `species`, without an interaction term.\n\n\n::: {.cell hash='11_exercises_cache/html/unnamed-chunk-3_5a93fda425e348c8d4e573a60d2f7c91'}\n\n```{.r .cell-code}\npeng_model <- stan_glm(body_mass_g ~ flipper_length_mm + species, \n                       data = penguin_data, \n                       family = gaussian,\n                       prior_intercept = normal(4000, 250),\n                       prior = normal(0, 2.5, autoscale = TRUE), \n                       prior_aux = exponential(1, autoscale = TRUE),\n                       chains = 4, iter = 5000*2, seed = 84735,\n                       prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.141 seconds (Warm-up)\nChain 1:                0.107 seconds (Sampling)\nChain 1:                0.248 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.118 seconds (Warm-up)\nChain 2:                0.142 seconds (Sampling)\nChain 2:                0.26 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.128 seconds (Warm-up)\nChain 3:                0.1 seconds (Sampling)\nChain 3:                0.228 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.122 seconds (Warm-up)\nChain 4:                0.112 seconds (Sampling)\nChain 4:                0.234 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nc. Create and interpret both visual and numerical diagnostics of your MCMC simulation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_predicted_draws(peng_model, n = 100) %>%\n  ggplot(aes(x = .prediction, group = .draw)) +\n    geom_density() + \n    xlab(\"body_mass_g\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \nIn add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`.\nUse the `ndraws` argument instead.\nSee help(\"tidybayes-deprecated\").\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\npenguin_data %>%\n  drop_na(flipper_length_mm, body_mass_g) %>% \n  add_fitted_draws(peng_model, n = 100) %>%\n  ggplot(aes(flipper_length_mm, body_mass_g, color = species)) +\n    geom_line(aes(y = .value, group = paste(species, .draw)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# MCMC diagnostics\nmcmc_trace(peng_model, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(peng_model)\n```\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(peng_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-4-5.png){width=672}\n:::\n:::\n\n\nd. Produce a `tidy()` summary of this model. Interpret the non-intercept coefficients’ posterior median values in context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior summary statistics\npeng_tidy <- tidy(peng_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %>% \n  select(-std.error)\n\npeng_tidy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  term              estimate conf.low conf.high\n  <chr>                <dbl>    <dbl>     <dbl>\n1 (Intercept)       3963.    -32649.     40497.\n2 flipper_length_mm    0.123   -181.       180.\n3 speciesGentoo        1.65   -5393.      5453.\n4 sigma              585.        99.7     1883.\n```\n:::\n\n```{.r .cell-code}\nas.data.frame(peng_model) %>% \n  mutate(adelie = `(Intercept)`, \n         gentoo = `(Intercept)` + speciesGentoo) %>% \n  mcmc_areas(pars = c(\"adelie\", \"gentoo\"))\n```\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\ne. Simulate, plot, and describe the posterior predictive model for the body mass of an Adelie penguin that has a flipper length of 197.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a set of predictions\nset.seed(84735)\nbm_predict <- posterior_predict(peng_model,\n                                newdata = data.frame(flipper_length_mm = 197, \n                                                     species = \"Adelie\"))\nmean(bm_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3996.982\n```\n:::\n\n```{.r .cell-code}\n# Plot the posterior predictive models\nmcmc_areas(bm_predict) +\n  scale_y_discrete(labels = \"Adelie\") + \n  xlab(\"body_mass_g\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n```\n:::\n\n::: {.cell-output-display}\n![](11_exercises_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}