{
  "hash": "46162c5a7801fd9de3ebeb4b7dc712d1",
  "result": {
    "markdown": "---\ntitle: \"9: Simple Normal Regression\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n```\n:::\n\n\n# 9.2 Tuning prior models for regression parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_normal(mean = 5000, sd = 1000) + \n  labs(x = \"beta_0c\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_normal(mean = 100, sd = 40) + \n  labs(x = \"beta_1\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_gamma(shape = 1, rate = 0.0008) + \n  labs(x = \"sigma\", y = \"pdf\")\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\n# 9.3 Posterior simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load and plot data\ndata(bikes)\n\nggplot(bikes, aes(x = temp_feel, y = rides)) + \n  geom_point(size = 1.5) + \n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 2)\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# 9.3.1 Simulation via rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_model <- stan_glm(rides ~ temp_feel, data = bikes,\n                       family = gaussian,\n                       prior_intercept = normal(5000, 1000),\n                       prior = normal(100, 40), \n                       prior_aux = exponential(0.0008),\n                       chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.248 seconds (Warm-up)\nChain 1:                0.274 seconds (Sampling)\nChain 1:                0.522 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.333 seconds (Warm-up)\nChain 2:                0.272 seconds (Sampling)\nChain 2:                0.605 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.23 seconds (Warm-up)\nChain 3:                0.264 seconds (Sampling)\nChain 3:                0.494 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.206 seconds (Warm-up)\nChain 4:                0.286 seconds (Sampling)\nChain 4:                0.492 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Effective sample size ratio and Rhat\nneff_ratio(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)   temp_feel       sigma \n    1.01655     1.00960     0.98635 \n```\n:::\n\n```{.r .cell-code}\nrhat(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)   temp_feel       sigma \n  0.9999477   0.9999072   0.9999549 \n```\n:::\n\n```{.r .cell-code}\n# Trace plots of parallel chains\nmcmc_trace(bike_model, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Density plots of parallel chains\nmcmc_dens_overlay(bike_model)\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsaveRDS(bike_model, \"bike_model.RDS\")\n```\n:::\n\n\n# 9.3.2 Optional: Simulation via rstan\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nstan_bike_model <- \"\n  data {\n    int<lower = 0> n;\n    vector[n] Y;\n    vector[n] X;\n  }\n  parameters {\n    real beta0;\n    real beta1;\n    real<lower = 0> sigma;\n  }\n  model {\n    Y ~ normal(beta0 + beta1 * X, sigma);\n    beta0 ~ normal(-2000, 1000);\n    beta1 ~ normal(100, 40);\n    sigma ~ exponential(0.0008);\n  }\n\"\n# STEP 2: SIMULATE the posterior\nstan_bike_sim <- \n  stan(model_code = stan_bike_model, \n       data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), \n       chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.437 seconds (Warm-up)\nChain 1:                1.532 seconds (Sampling)\nChain 1:                2.969 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.509 seconds (Warm-up)\nChain 2:                1.697 seconds (Sampling)\nChain 2:                3.206 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.535 seconds (Warm-up)\nChain 3:                1.655 seconds (Sampling)\nChain 3:                3.19 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.606 seconds (Warm-up)\nChain 4:                1.521 seconds (Sampling)\nChain 4:                3.127 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\n# 9.4 Interpreting the posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior summary statistics\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  -2195.     359.    -2657.    -1741. \n2 temp_feel       82.2      5.05     75.7      88.8\n3 sigma         1282.      40.6    1232.     1336. \n4 mean_PPD      3487.      80.8    3383.     3590. \n```\n:::\n\n```{.r .cell-code}\n# Store the 4 chains for each parameter in 1 data frame\nbike_model_df <- as.data.frame(bike_model)\n\n# Check it out\nnrow(bike_model_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20000\n```\n:::\n\n```{.r .cell-code}\nhead(bike_model_df, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) temp_feel    sigma\n1   -2593.578  87.59029 1362.773\n2   -2207.766  82.06586 1236.125\n3   -2515.712  86.25428 1329.203\n```\n:::\n\n```{.r .cell-code}\n# 50 simulated model lines\nbikes %>%\n  add_fitted_draws(bike_model, n = 50) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + \n    geom_point(data = bikes, size = 0.05)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Tabulate the beta_1 values that exceed 0\nbike_model_df %>% \n  mutate(exceeds_0 = temp_feel > 0) %>% \n  tabyl(exceeds_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n exceeds_0     n percent\n      TRUE 20000       1\n```\n:::\n\n```{.r .cell-code}\n# Simulate four sets of data\nbikes %>%\n  add_predicted_draws(bike_model, n = 4) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_point(aes(y = .prediction, group = .draw), size = 0.2) + \n    facet_wrap(~ .draw)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \nIn add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`.\nUse the `ndraws` argument instead.\nSee help(\"tidybayes-deprecated\").\n```\n:::\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsaveRDS(bike_model_df, \"bike_model_df.RDS\")\n```\n:::\n\n\n# 9.5.1 Building a posterior predictive model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_set <- head(bike_model_df, 1)\nfirst_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) temp_feel    sigma\n1   -2593.578  87.59029 1362.773\n```\n:::\n\n```{.r .cell-code}\nmu <- first_set$`(Intercept)` + first_set$temp_feel * 75\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3975.694\n```\n:::\n\n```{.r .cell-code}\nset.seed(84735)\ny_new <- rnorm(1, mean = mu, sd = first_set$sigma)\ny_new\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4884.981\n```\n:::\n\n```{.r .cell-code}\n# Predict rides for each parameter set in the chain\nset.seed(84735)\npredict_75 <- bike_model_df %>% \n  mutate(mu = `(Intercept)` + temp_feel*75,\n         y_new = rnorm(20000, mean = mu, sd = sigma))\n\nhead(predict_75, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) temp_feel    sigma       mu    y_new\n1   -2593.578  87.59029 1362.773 3975.694 4884.981\n2   -2207.766  82.06586 1236.125 3947.173 3794.091\n3   -2515.712  86.25428 1329.203 3953.359 4991.291\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct 80% posterior credible intervals\npredict_75 %>% \n  summarize(lower_mu = quantile(mu, 0.025),\n            upper_mu = quantile(mu, 0.975),\n            lower_new = quantile(y_new, 0.025),\n            upper_new = quantile(y_new, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  lower_mu upper_mu lower_new upper_new\n1 3842.508 4093.436  1492.104  6488.281\n```\n:::\n\n```{.r .cell-code}\n# Plot the posterior model of the typical ridership on 75 degree days\nggplot(predict_75, aes(x = mu)) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot the posterior predictive model of tomorrow's ridership\nggplot(predict_75, aes(x = y_new)) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\n# 9.5.2 Posterior prediction with rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate a set of predictions\nset.seed(84735)\nshortcut_prediction <- \n  posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))\n\n# Construct a 95% posterior credible interval\nposterior_interval(shortcut_prediction, prob = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      2.5%    97.5%\n1 1492.104 6488.281\n```\n:::\n\n```{.r .cell-code}\n# Plot the approximate predictive model\nmcmc_dens(shortcut_prediction) + \n  xlab(\"predicted ridership on a 75 degree day\")\n```\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n# 9.6 Sequential regression modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>% \n  select(date, temp_feel, rides) %>% \n  head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        date temp_feel rides\n1 2011-01-01  64.72625   654\n2 2011-01-03  49.04645  1229\n3 2011-01-04  51.09098  1454\n```\n:::\n\n```{.r .cell-code}\nphase_1 <- bikes[1:30, ]\nphase_2 <- bikes[1:60, ]\nphase_3 <- bikes\n\nmy_model <- stan_glm(rides ~ temp_feel, data = phase_1, family = gaussian, \n                     prior_intercept = normal(5000, 1000),\n                     prior = normal(100, 40), \n                     prior_aux = exponential(0.0008),\n                     chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.297 seconds (Warm-up)\nChain 1:                0.142 seconds (Sampling)\nChain 1:                0.439 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.27 seconds (Warm-up)\nChain 2:                0.152 seconds (Sampling)\nChain 2:                0.422 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.308 seconds (Warm-up)\nChain 3:                0.164 seconds (Sampling)\nChain 3:                0.472 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.277 seconds (Warm-up)\nChain 4:                0.157 seconds (Sampling)\nChain 4:                0.434 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\n# 9.7 Using default rstanarm priors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_model_default <- stan_glm(\n  rides ~ temp_feel, data = bikes, \n  family = gaussian,\n  prior_intercept = normal(5000, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.218 seconds (Warm-up)\nChain 1:                0.308 seconds (Sampling)\nChain 1:                0.526 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.243 seconds (Warm-up)\nChain 2:                0.3 seconds (Sampling)\nChain 2:                0.543 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.27 seconds (Warm-up)\nChain 3:                0.288 seconds (Sampling)\nChain 3:                0.558 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.304 seconds (Warm-up)\nChain 4:                0.291 seconds (Sampling)\nChain 4:                0.595 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nprior_summary(bike_model_default)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'bike_model_default' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 5000, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 5000, scale = 3937)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 351)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.00064)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\n# Perform a prior simulation \nbike_default_priors <- update(bike_model_default, prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.802 seconds (Warm-up)\nChain 1:                0.166 seconds (Sampling)\nChain 1:                0.968 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.974 seconds (Warm-up)\nChain 2:                0.104 seconds (Sampling)\nChain 2:                1.078 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.772 seconds (Warm-up)\nChain 3:                0.098 seconds (Sampling)\nChain 3:                0.87 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.31 seconds (Warm-up)\nChain 4:                0.156 seconds (Sampling)\nChain 4:                1.466 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# 200 prior model lines\nbikes %>%\n  add_fitted_draws(bike_default_priors, n = 200) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# 4 prior simulated datasets\nset.seed(3)\nbikes %>%\n  add_predicted_draws(bike_default_priors, n = 4) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_point(aes(y = .prediction, group = .draw)) + \n    facet_wrap(~ .draw)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \nIn add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`.\nUse the `ndraws` argument instead.\nSee help(\"tidybayes-deprecated\").\n```\n:::\n\n::: {.cell-output-display}\n![](09_notes_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "09_notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}