{
  "hash": "2d567955b68b30300abec91facf226c9",
  "result": {
    "markdown": "---\ntitle: \"12: Poisson & Negative Binomial Regression\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\ncolors <- c(\"#7400CC\", \"#CC0AA4\", \"#3ACC14\", \"#0E0ACC\", \"#CCAC14\", \n            \"#CC5514\", \"#0ACCC5\")\n\n# Load data\ndata(equality_index)\nequality <- equality_index\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(equality, aes(x = laws)) + \n  geom_histogram(color = \"white\", breaks = seq(0, 160, by = 10))\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Identify the outlier\nequality %>% \n  filter(laws == max(laws))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  state      region gop_2016  laws historical percent_urban\n  <fct>      <fct>     <dbl> <dbl> <fct>              <dbl>\n1 california west       31.6   155 dem                   95\n```\n:::\n\n```{.r .cell-code}\n# Remove the outlier\nequality <- equality %>% \n  filter(state != \"california\")\n\nggplot(equality, aes(y = laws, x = percent_urban, color = historical)) + \n  geom_point() +\n  scale_color_manual(values = colors)\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-2_866019cba70ec568a46c81a26a12bccd'}\n\n```{.r .cell-code}\n# Simulate the Normal model\nequality_normal_sim <- stan_glm(laws ~ percent_urban + historical, \n                                data = equality, \n                                family = gaussian,\n                                prior_intercept = normal(7, 1.5),\n                                prior = normal(0, 2.5, autoscale = TRUE),\n                                prior_aux = exponential(1, autoscale = TRUE),\n                                chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.18 seconds (Warm-up)\nChain 1:                0.239 seconds (Sampling)\nChain 1:                0.419 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.2 seconds (Warm-up)\nChain 2:                0.233 seconds (Sampling)\nChain 2:                0.433 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.181 seconds (Warm-up)\nChain 3:                0.189 seconds (Sampling)\nChain 3:                0.37 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.176 seconds (Warm-up)\nChain 4:                0.238 seconds (Sampling)\nChain 4:                0.414 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Posterior predictive check\npp_check(equality_normal_sim, plotfun = \"hist\", nreps = 5) + \n  geom_vline(xintercept = 0) + \n  xlab(\"laws\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n# 12.1 Building the Poisson regression model\n\n## 12.1.2 Specifying the priors\n\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-3_08ffef548491ccd1922008affc87e489'}\n\n```{.r .cell-code}\nequality_model_prior <- stan_glm(laws ~ percent_urban + historical, \n                                 data = equality, \n                                 family = poisson,\n                                 prior_intercept = normal(2, 0.5),\n                                 prior = normal(0, 2.5, autoscale = TRUE), \n                                 chains = 4, iter = 5000*2, seed = 84735, \n                                 prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.111 seconds (Warm-up)\nChain 1:                0.099 seconds (Sampling)\nChain 1:                0.21 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.085 seconds (Warm-up)\nChain 2:                0.088 seconds (Sampling)\nChain 2:                0.173 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.089 seconds (Warm-up)\nChain 3:                0.103 seconds (Sampling)\nChain 3:                0.192 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.111 seconds (Warm-up)\nChain 4:                0.117 seconds (Sampling)\nChain 4:                0.228 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nprior_summary(equality_model_prior)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'equality_model_prior' \n------\nIntercept (after predictors centered)\n ~ normal(location = 2, scale = 0.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0,0], scale = [2.5,2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0,0], scale = [0.17,4.97,5.60])\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\nequality %>% \n  add_fitted_draws(equality_model_prior, n = 100) %>%\n  ggplot(aes(x = percent_urban, y = laws, color = historical)) +\n    geom_line(aes(y = .value, group = paste(historical, .draw))) + \n    ylim(0, 100) +\n  scale_color_manual(values = colors)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1298 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# 12.2 Simulating the posterior\n\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-4_4e53bbc19c2c98d26f66f819fe2c9061'}\n\n```{.r .cell-code}\nequality_model <- update(equality_model_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.454 seconds (Warm-up)\nChain 1:                0.516 seconds (Sampling)\nChain 1:                0.97 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.454 seconds (Warm-up)\nChain 2:                0.47 seconds (Sampling)\nChain 2:                0.924 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.447 seconds (Warm-up)\nChain 3:                0.488 seconds (Sampling)\nChain 3:                0.935 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.456 seconds (Warm-up)\nChain 4:                0.495 seconds (Sampling)\nChain 4:                0.951 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nmcmc_trace(equality_model)\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(equality_model)\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(equality_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\nset.seed(1)\npp_check(equality_model, plotfun = \"hist\", nreps = 5) + \n  xlab(\"laws\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n```{.r .cell-code}\npp_check(equality_model) + \n  xlab(\"laws\")\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-4-5.png){width=672}\n:::\n:::\n\n\n# 12.3 Interpreting the posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequality %>%\n  add_fitted_draws(equality_model, n = 50) %>%\n  ggplot(aes(x = percent_urban, y = laws, color = historical)) +\n    geom_line(aes(y = .value, group = paste(historical, .draw)), \n              alpha = .1) +\n    geom_point(data = equality, size = 0.1) +\n  scale_color_manual(values = colors)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntidy(equality_model, conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term            estimate std.error conf.low conf.high\n  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)       1.71     0.303     1.31      2.09  \n2 percent_urban     0.0164   0.00353   0.0119    0.0210\n3 historicalgop    -1.52     0.134    -1.69     -1.34  \n4 historicalswing  -0.610    0.103    -0.745    -0.477 \n```\n:::\n:::\n\n\n# 12.4 Posterior prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequality %>% \n  filter(state == \"minnesota\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  state     region  gop_2016  laws historical percent_urban\n  <fct>     <fct>      <dbl> <dbl> <fct>              <dbl>\n1 minnesota midwest     44.9     4 dem                 73.3\n```\n:::\n\n```{.r .cell-code}\n# Calculate posterior predictions\nset.seed(84735)\nmn_prediction <- posterior_predict(\n  equality_model, newdata = data.frame(percent_urban = 73.3, \n                                       historical = \"dem\"))\nhead(mn_prediction, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      1\n[1,] 20\n[2,] 17\n[3,] 21\n```\n:::\n\n```{.r .cell-code}\nmcmc_hist(mn_prediction, binwidth = 1) + \n  geom_vline(xintercept = 4) + \n  xlab(\"Predicted number of laws in Minnesota\")\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Predict number of laws for each parameter set in the chain\nset.seed(84735)\nas.data.frame(equality_model) %>% \n  mutate(log_lambda = `(Intercept)` + percent_urban*73.3 + \n           historicalgop*0 + historicalswing*0,\n         lambda = exp(log_lambda),\n         y_new = rpois(20000, lambda = lambda)) %>% \n  ggplot(aes(x = y_new)) + \n    stat_count()\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n# 12.5 Model evaluation\n\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-7_6992c903f007a450215c9c34ff421293'}\n\n```{.r .cell-code}\n# Simulate posterior predictive models for each state\nset.seed(84735)\npoisson_predictions <- posterior_predict(equality_model, newdata = equality)\n\n# Plot the posterior predictive models for each state\nppc_intervals_grouped(equality$laws, yrep = poisson_predictions, \n                      x = equality$percent_urban, \n                      group = equality$historical,\n                      prob = 0.5, prob_outer = 0.95,\n                      facet_args = list(scales = \"fixed\"))\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprediction_summary(model = equality_model, data = equality)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mae mae_scaled within_50 within_95\n1 3.3824   1.301441 0.3265306 0.7755102\n```\n:::\n\n```{.r .cell-code}\n# Cross-validation\nset.seed(84735)\npoisson_cv <- prediction_summary_cv(model = equality_model, \n                                    data = equality, k = 10)\n```\n:::\n\n\n# 12.6 Negative Binomial regression for overdispersed counts\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndata(pulse_of_the_nation)\npulse <- pulse_of_the_nation %>% \n  filter(books < 100)\n\nggplot(pulse, aes(x = books)) + \n  geom_histogram(color = \"white\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(pulse, aes(y = books, x = age)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(pulse, aes(y = books, x = wise_unwise)) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-9_c6c0174325a6b78414ea687828e99e86'}\n\n```{.r .cell-code}\nbooks_poisson_sim <- stan_glm(\n  books ~ age + wise_unwise, \n  data = pulse, family = poisson,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000182 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.82 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 6.176 seconds (Warm-up)\nChain 1:                7.23 seconds (Sampling)\nChain 1:                13.406 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000177 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.77 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 6.233 seconds (Warm-up)\nChain 2:                6.785 seconds (Sampling)\nChain 2:                13.018 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000175 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.75 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 6.129 seconds (Warm-up)\nChain 3:                6.722 seconds (Sampling)\nChain 3:                12.851 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000177 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.77 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 6.159 seconds (Warm-up)\nChain 4:                6.931 seconds (Sampling)\nChain 4:                13.09 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\npp_check(books_poisson_sim) + \n  xlab(\"books\")\n```\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Mean and variability in readership across all subjects\npulse %>% \n  summarize(mean = mean(books), var = var(books))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n   mean   var\n  <dbl> <dbl>\n1  10.9  198.\n```\n:::\n:::\n\n::: {.cell hash='12_notes_cache/html/unnamed-chunk-10_bbd51ec70ece810d883a81fa26209004'}\n\n```{.r .cell-code}\nbooks_negbin_sim <- stan_glm(\n  books ~ age + wise_unwise, \n  data = pulse, family = neg_binomial_2,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000515 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 19.123 seconds (Warm-up)\nChain 1:                20.409 seconds (Sampling)\nChain 1:                39.532 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000659 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 6.59 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 19.058 seconds (Warm-up)\nChain 2:                20.671 seconds (Sampling)\nChain 2:                39.729 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000514 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 5.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 19.145 seconds (Warm-up)\nChain 3:                21.484 seconds (Sampling)\nChain 3:                40.629 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000519 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 5.19 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 19.135 seconds (Warm-up)\nChain 4:                21.271 seconds (Sampling)\nChain 4:                40.406 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Check out the priors\nprior_summary(books_negbin_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'books_negbin_sim' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.15,5.01])\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\npp_check(books_negbin_sim) + \n  xlim(0, 75) + \n  xlab(\"books\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 193 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 3 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12_notes_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Numerical summaries\ntidy(books_negbin_sim, conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term                        estimate std.error conf.low conf.high\n  <chr>                          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)                 2.23       0.131    2.07      2.41   \n2 age                         0.000365   0.00239 -0.00270   0.00339\n3 wise_unwiseWise but Unhappy 0.266      0.0798   0.162     0.368  \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}