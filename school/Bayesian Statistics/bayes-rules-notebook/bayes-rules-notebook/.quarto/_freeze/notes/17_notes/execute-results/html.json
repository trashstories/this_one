{
  "hash": "2368251fc8d4e9733936b33c17f1ba68",
  "result": {
    "markdown": "---\ntitle: \"17: (Normal) Hierarchical Models with Predictors\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(broom.mixed)\n\n# Load data\ndata(cherry_blossom_sample)\nrunning <- cherry_blossom_sample\n\n# Remove NAs\nrunning <- running %>% \n  select(runner, age, net) %>% \n  na.omit()\n```\n:::\n\n\n# 17.1 First steps: Complete pooling\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete_pooled_model <- read_rds(\"complete_pooled_model.rds\")\n\n# Posterior summary statistics\nmodel_summary <- tidy(complete_pooled_model, \n                      conf.int = TRUE, conf.level = 0.80)\nmodel_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   75.2      24.6     43.7     106.   \n2 age            0.268     0.446   -0.302     0.842\n```\n:::\n\n```{.r .cell-code}\n# Posterior median model\nB0 <- model_summary$estimate[1]\nB1 <- model_summary$estimate[2]\nggplot(running, aes(x = age, y = net)) + \n  geom_point() + \n  geom_abline(aes(intercept = B0, slope = B1))\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## 17.2.3 Tuning the prior\n\n\n::: {.cell hash='17_notes_cache/html/unnamed-chunk-3_fbe376e7048cf7dd621eec3221b1efe2'}\n\n```{.r .cell-code}\nrunning_model_1_prior <- stan_glmer(\n  net ~ age + (1 | runner), \n  data = running, family = gaussian,\n  prior_intercept = normal(100, 10),\n  prior = normal(2.5, 1), \n  prior_aux = exponential(1, autoscale = TRUE),\n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735, \n  prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.423 seconds (Warm-up)\nChain 1:                0.452 seconds (Sampling)\nChain 1:                0.875 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.416 seconds (Warm-up)\nChain 2:                0.45 seconds (Sampling)\nChain 2:                0.866 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.432 seconds (Warm-up)\nChain 3:                0.454 seconds (Sampling)\nChain 3:                0.886 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.428 seconds (Warm-up)\nChain 4:                0.442 seconds (Sampling)\nChain 4:                0.87 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nset.seed(84735)\nrunning %>% \n  add_fitted_draws(running_model_1_prior, n = 4) %>%\n  ggplot(aes(x = age, y = net)) +\n    geom_line(aes(y = .value, group = paste(runner, .draw))) + \n    facet_wrap(~ .draw)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nrunning %>%\n  add_predicted_draws(running_model_1_prior, n = 100) %>%\n  ggplot(aes(x = net)) +\n    geom_density(aes(x = .prediction, group = .draw)) +\n    xlim(-100,300)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: \nIn add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`.\nUse the `ndraws` argument instead.\nSee help(\"tidybayes-deprecated\").\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 70 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n## 17.2.4 Posterior simulation & analysis\n\n\n::: {.cell hash='17_notes_cache/html/unnamed-chunk-4_dfb76cc5f01b24cf4cb290fa5af804a6'}\n\n```{.r .cell-code}\nggplot(running, aes(x = age, y = net)) + \n  geom_point() + \n  facet_wrap(~ runner)\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Simulate the posterior\nrunning_model_1 <- update(running_model_1_prior, prior_PD = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.102 seconds (Warm-up)\nChain 1:                1.801 seconds (Sampling)\nChain 1:                3.903 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.958 seconds (Warm-up)\nChain 2:                1.867 seconds (Sampling)\nChain 2:                3.825 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 2.062 seconds (Warm-up)\nChain 3:                1.867 seconds (Sampling)\nChain 3:                3.929 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 2.006 seconds (Warm-up)\nChain 4:                2.091 seconds (Sampling)\nChain 4:                4.097 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# Check the prior specifications\nprior_summary(running_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'running_model_1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 100, scale = 10)\n\nCoefficients\n ~ normal(location = 2.5, scale = 1)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.072)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n\n```{.r .cell-code}\n# Markov chain diagnostics\nmcmc_trace(running_model_1)\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(running_model_1)\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(running_model_1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n```{.r .cell-code}\nneff_ratio(running_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          (Intercept)                                   age \n                              0.60435                               0.73065 \n              b[(Intercept) runner:1]               b[(Intercept) runner:2] \n                              0.16370                               0.16170 \n              b[(Intercept) runner:3]               b[(Intercept) runner:4] \n                              0.15370                               0.17225 \n              b[(Intercept) runner:5]               b[(Intercept) runner:6] \n                              0.15995                               0.19280 \n              b[(Intercept) runner:7]               b[(Intercept) runner:8] \n                              0.17015                               0.18920 \n              b[(Intercept) runner:9]              b[(Intercept) runner:10] \n                              0.16825                               0.17075 \n             b[(Intercept) runner:11]              b[(Intercept) runner:12] \n                              0.16425                               0.17910 \n             b[(Intercept) runner:13]              b[(Intercept) runner:14] \n                              0.16865                               0.14030 \n             b[(Intercept) runner:15]              b[(Intercept) runner:16] \n                              0.16240                               0.16970 \n             b[(Intercept) runner:17]              b[(Intercept) runner:18] \n                              0.16615                               0.18790 \n             b[(Intercept) runner:19]              b[(Intercept) runner:20] \n                              0.18155                               0.16565 \n             b[(Intercept) runner:21]              b[(Intercept) runner:22] \n                              0.15140                               0.16935 \n             b[(Intercept) runner:23]              b[(Intercept) runner:24] \n                              0.15615                               0.15990 \n             b[(Intercept) runner:25]              b[(Intercept) runner:26] \n                              0.16740                               0.16480 \n             b[(Intercept) runner:27]              b[(Intercept) runner:28] \n                              0.15965                               0.16920 \n             b[(Intercept) runner:29]              b[(Intercept) runner:30] \n                              0.15885                               0.16350 \n             b[(Intercept) runner:31]              b[(Intercept) runner:32] \n                              0.16800                               0.17200 \n             b[(Intercept) runner:33]              b[(Intercept) runner:34] \n                              0.19645                               0.16845 \n             b[(Intercept) runner:35]              b[(Intercept) runner:36] \n                              0.15115                               0.18770 \n                                sigma Sigma[runner:(Intercept),(Intercept)] \n                              0.63670                               0.12690 \n```\n:::\n\n```{.r .cell-code}\nrhat(running_model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          (Intercept)                                   age \n                            0.9999016                             0.9998563 \n              b[(Intercept) runner:1]               b[(Intercept) runner:2] \n                            1.0006624                             1.0005236 \n              b[(Intercept) runner:3]               b[(Intercept) runner:4] \n                            1.0006296                             1.0003231 \n              b[(Intercept) runner:5]               b[(Intercept) runner:6] \n                            1.0004240                             1.0002937 \n              b[(Intercept) runner:7]               b[(Intercept) runner:8] \n                            1.0004896                             1.0003863 \n              b[(Intercept) runner:9]              b[(Intercept) runner:10] \n                            1.0005383                             1.0002673 \n             b[(Intercept) runner:11]              b[(Intercept) runner:12] \n                            1.0004497                             1.0004914 \n             b[(Intercept) runner:13]              b[(Intercept) runner:14] \n                            1.0005423                             1.0003792 \n             b[(Intercept) runner:15]              b[(Intercept) runner:16] \n                            1.0004496                             1.0004662 \n             b[(Intercept) runner:17]              b[(Intercept) runner:18] \n                            1.0003301                             1.0003634 \n             b[(Intercept) runner:19]              b[(Intercept) runner:20] \n                            1.0001792                             1.0002641 \n             b[(Intercept) runner:21]              b[(Intercept) runner:22] \n                            1.0005621                             1.0002860 \n             b[(Intercept) runner:23]              b[(Intercept) runner:24] \n                            1.0005896                             1.0003012 \n             b[(Intercept) runner:25]              b[(Intercept) runner:26] \n                            1.0004223                             1.0005745 \n             b[(Intercept) runner:27]              b[(Intercept) runner:28] \n                            1.0003865                             1.0003909 \n             b[(Intercept) runner:29]              b[(Intercept) runner:30] \n                            1.0005363                             1.0003641 \n             b[(Intercept) runner:31]              b[(Intercept) runner:32] \n                            1.0003867                             1.0004710 \n             b[(Intercept) runner:33]              b[(Intercept) runner:34] \n                            1.0002443                             1.0003710 \n             b[(Intercept) runner:35]              b[(Intercept) runner:36] \n                            1.0003742                             1.0004276 \n                                sigma Sigma[runner:(Intercept),(Intercept)] \n                            0.9999669                             1.0010135 \n```\n:::\n:::\n\n\n### 17.2.4.1 Posterior analysis of the global relationship\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_summary_1 <- tidy(running_model_1, effects = \"fixed\",\n                       conf.int = TRUE, conf.level = 0.80)\ntidy_summary_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    19.1     12.3       3.35     34.8 \n2 age             1.30     0.221     1.02      1.58\n```\n:::\n\n```{.r .cell-code}\nB0 <- tidy_summary_1$estimate[1]\nB1 <- tidy_summary_1$estimate[2]\nrunning %>%\n  add_fitted_draws(running_model_1, n = 200, re_formula = NA) %>%\n  ggplot(aes(x = age, y = net)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.1) +\n    geom_abline(intercept = B0, slope = B1, color = \"blue\") +\n    lims(y = c(75, 110))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### 17.2.4.2 Posterior analysis of group-specific relationships\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior summaries of runner-specific intercepts\nrunner_summaries_1 <- running_model_1 %>%\n  spread_draws(`(Intercept)`, b[,runner]) %>% \n  mutate(runner_intercept = `(Intercept)` + b) %>% \n  select(-`(Intercept)`, -b) %>% \n  median_qi(.width = 0.80) %>% \n  select(runner, runner_intercept, .lower, .upper)\n\nrunner_summaries_1 %>% \n  filter(runner %in% c(\"runner:4\", \"runner:5\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  runner   runner_intercept .lower .upper\n  <chr>               <dbl>  <dbl>  <dbl>\n1 runner:4            31.0   15.1    46.7\n2 runner:5             6.86  -8.62   22.1\n```\n:::\n\n```{.r .cell-code}\n# 100 posterior plausible models for runners 4 & 5\nrunning %>%\n  filter(runner %in% c(\"4\", \"5\")) %>% \n  add_fitted_draws(running_model_1, n = 100) %>%\n  ggplot(aes(x = age, y = net)) +\n    geom_line(\n      aes(y = .value, group = paste(runner, .draw), color = runner),\n      alpha = 0.1) +\n    geom_point(aes(color = runner))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n- Use [add_]epred_draws() to get the expectation of the posterior predictive.\n- Use [add_]linpred_draws() to get the distribution of the linear predictor.\n- For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n  means you most likely want [add_]epred_draws(...).\nNOTE: When updating to the new functions, note that the `model` parameter is now\n  named `object` and the `n` parameter is now named `ndraws`.\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot runner-specific models with the global model\nggplot(running, aes(y = net, x = age, group = runner)) + \n  geom_abline(data = runner_summaries_1, color = \"gray\",\n              aes(intercept = runner_intercept, slope = B1)) + \n  geom_abline(intercept = B0, slope = B1, color = \"blue\") + \n  lims(x = c(50, 61), y = c(50, 135))\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n### 17.2.4.3 Posterior analysis of within- and between-group variability\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_sigma <- tidy(running_model_1, effects = \"ran_pars\")\ntidy_sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  term                    group    estimate\n  <chr>                   <chr>       <dbl>\n1 sd_(Intercept).runner   runner      13.3 \n2 sd_Observation.Residual Residual     5.25\n```\n:::\n\n```{.r .cell-code}\nsigma_0 <- tidy_sigma[1,3]\nsigma_y <- tidy_sigma[2,3]\nsigma_0^2 / (sigma_0^2 + sigma_y^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   estimate\n1 0.8651856\n```\n:::\n\n```{.r .cell-code}\nsigma_y^2 / (sigma_0^2 + sigma_y^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   estimate\n1 0.1348144\n```\n:::\n:::\n\n\n# 17.3 Hierarchical model with varying intercepts & slopes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot runner-specific models in the data\nrunning %>% \n  filter(runner %in% c(\"4\", \"5\", \"20\", \"29\")) %>% \n  ggplot(., aes(x = age, y = net)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) + \n    facet_grid(~ runner)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(running, aes(x = age, y = net, group = runner)) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\n## 17.3.3 Posterior simulation & analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \n# running_model_2 <- stan_glmer(\n#   net ~ age + (age | runner),\n#   data = running, family = gaussian,\n#   prior_intercept = normal(100, 10),\n#   prior = normal(2.5, 1), \n#   prior_aux = exponential(1, autoscale = TRUE),\n#   prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n#   chains = 4, iter = 5000*2, seed = 84735, adapt_delta = 0.99999\n# )\n# \n# saveRDS(running_model_2, \"running_model_2.rds\")\n\nrunning_model_2 <- readRDS(\"~/MPP/2023-06 Summer/Bayesian Statistics/bayes-rules-notebook/bayes-rules-notebook/running_model_2.rds\")\n\n# Confirm the prior model specifications\nprior_summary(running_model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'running_model_2' \n------\nIntercept (after predictors centered)\n ~ normal(location = 100, scale = 10)\n\nCoefficients\n ~ normal(location = 2.5, scale = 1)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.072)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n\n### 17.3.3.1 Posterior analysis of the global and group-specific parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quick summary of global regression parameters\ntidy(running_model_2, effects = \"fixed\", conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    18.5     11.7       3.29     33.8 \n2 age             1.31     0.216     1.03      1.60\n```\n:::\n\n```{.r .cell-code}\n# Get MCMC chains for the runner-specific intercepts & slopes\nrunner_chains_2 <- running_model_2 %>%\n  spread_draws(`(Intercept)`, b[term, runner], `age`) %>% \n  pivot_wider(names_from = term, names_glue = \"b_{term}\",\n              values_from = b) %>% \n  mutate(runner_intercept = `(Intercept)` + `b_(Intercept)`,\n         runner_age = age + b_age)\n\n# Posterior medians of runner-specific models\nrunner_summaries_2 <- runner_chains_2 %>% \n  group_by(runner) %>% \n  summarize(runner_intercept = median(runner_intercept),\n            runner_age = median(runner_age))\n\n# Check it out\nhead(runner_summaries_2, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  runner    runner_intercept runner_age\n  <chr>                <dbl>      <dbl>\n1 runner:1              18.6       1.06\n2 runner:10             18.6       1.75\n3 runner:11             18.5       1.32\n```\n:::\n\n```{.r .cell-code}\nggplot(running, aes(y = net, x = age, group = runner)) + \n  geom_abline(data = runner_summaries_2, color = \"gray\",\n              aes(intercept = runner_intercept, slope = runner_age)) + \n  lims(x = c(50, 61), y = c(50, 135))\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### 17.3.3.2 Posterior analysis of within- and between-group variability\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(running_model_2, effects = \"ran_pars\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  term                       group    estimate\n  <chr>                      <chr>       <dbl>\n1 sd_(Intercept).runner      runner     1.40  \n2 sd_age.runner              runner     0.251 \n3 cor_(Intercept).age.runner runner    -0.0991\n4 sd_Observation.Residual    Residual   5.16  \n```\n:::\n:::\n\n\n# 17.4 Model evaluation & selection\n\n\n::: {.cell hash='17_notes_cache/html/unnamed-chunk-12_29c55bae645ffa86b29d13bad19808d4'}\n\n```{.r .cell-code}\npp_check(complete_pooled_model) + \n  labs(x = \"net\", title = \"complete pooled model\")\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\npp_check(running_model_1) + \n  labs(x = \"net\", title = \"running model 1\")\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\npp_check(running_model_2) + \n  labs(x = \"net\", title = \"running model 2\")\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate prediction summaries\nset.seed(84735)\nprediction_summary(model = running_model_1, data = running)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mae mae_scaled within_50 within_95\n1 2.617258   0.455396 0.6918919  0.972973\n```\n:::\n\n```{.r .cell-code}\nprediction_summary(model = running_model_2, data = running)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mae mae_scaled within_50 within_95\n1 2.551753  0.4439288 0.7135135  0.972973\n```\n:::\n\n```{.r .cell-code}\nprediction_summary_cv(model = running_model_1, data = running,\n                      k = 10, group = \"runner\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$folds\n   fold       mae mae_scaled within_50 within_95\n1     1 12.253328  0.8461246 0.2142857 1.0000000\n2     2 10.409570  0.7182196 0.4736842 0.8947368\n3     3  6.937431  0.4695956 0.6428571 1.0000000\n4     4  8.663300  0.5848710 0.5217391 1.0000000\n5     5  9.825064  0.6912654 0.4545455 0.7272727\n6     6 19.084829  1.3818222 0.1250000 0.6250000\n7     7 14.160507  0.9749042 0.2105263 1.0000000\n8     8  5.953543  0.3963053 0.6470588 1.0000000\n9     9 12.148999  0.8527325 0.4090909 0.9545455\n10   10  6.484942  0.4335957 0.6315789 1.0000000\n\n$cv\n       mae mae_scaled within_50 within_95\n1 10.59215  0.7349436 0.4330367 0.9201555\n```\n:::\n\n```{.r .cell-code}\n# Calculate ELPD for the 2 models\nelpd_hierarchical_1 <- loo(running_model_1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Found 1 observation(s) with a pareto_k > 0.7. We recommend calling 'loo' again with argument 'k_threshold = 0.7' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations directly.\n```\n:::\n\n```{.r .cell-code}\nelpd_hierarchical_2 <- loo(running_model_2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Found 1 observation(s) with a pareto_k > 0.7. We recommend calling 'loo' again with argument 'k_threshold = 0.7' in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 1 times to compute the ELPDs for the problematic observations directly.\n```\n:::\n\n```{.r .cell-code}\n# Compare the ELPD\nloo_compare(elpd_hierarchical_1, elpd_hierarchical_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                elpd_diff se_diff\nrunning_model_2  0.0       0.0   \nrunning_model_1 -1.5       1.2   \n```\n:::\n:::\n\n\n# 17.5 Posterior prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot runner-specific# Plot runner-specific trends for runners 1 & 10\nrunning %>% \n  filter(runner %in% c(\"1\", \"10\")) %>% \n  ggplot(., aes(x = age, y = net)) + \n    geom_point() + \n    facet_grid(~ runner) + \n    lims(x = c(54, 61))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Simulate posterior predictive models for the 3 runners\nset.seed(84735)\npredict_next_race <- posterior_predict(\n  running_model_1, \n  newdata = data.frame(runner = c(\"1\", \"Miles\", \"10\"),\n                       age = c(61, 61, 61)))\n\nB0 + B1 * 61\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 98.30617\n```\n:::\n\n```{.r .cell-code}\n# Posterior predictive model plots\nmcmc_areas(predict_next_race, prob = 0.8) +\n ggplot2::scale_y_discrete(labels = c(\"runner 1\", \"Miles\", \"runner 10\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n\n# 17.7 Example: Danceability\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import and wrangle the data\ndata(spotify)\nspotify <- spotify %>% \n  select(artist, title, danceability, valence, genre)\n\nggplot(spotify, aes(y = danceability, x = genre)) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(spotify, aes(y = danceability, x = valence)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(spotify, aes(y = danceability, x = valence, group = artist)) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](17_notes_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}