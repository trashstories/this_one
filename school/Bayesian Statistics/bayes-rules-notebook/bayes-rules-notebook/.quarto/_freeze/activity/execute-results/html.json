{
  "hash": "f965c78c7ba41d4968506abf3b12b156",
  "result": {
    "markdown": "---\ntitle: Bayes Rules! activity\noutput:\n  rmarkdown::html_document:\n    toc: true\n    toc_float: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\\\n\n\n# Getting started \n\n**GOALS**\n\n- Learn to think like Bayesians.\n- Apply Bayesian thinking in a regression setting.\n\n\n\\\n\n\n**DIRECTIONS**    \n\nYou do you. Depending upon your previous Bayes and modeling experience, you might choose to skip around.\n\n\n\\\n\n\n\n**GET SET UP**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(bayesrules)\nlibrary(bayesplot)\nlibrary(rstanarm)\nlibrary(broom.mixed)\n```\n:::\n\n\n\n\n\\\n\\\n\n\n\n# Part 1: Thinking like a Bayesian\n\n**GOAL**    \n\nIn this first set of exercises, you'll use functions in the `bayesrules` package to familiarize yourself with the main components of a Bayesian model: the prior, data, and posterior. You'll do so in the context of \"Beta-Binomial\" model.\n\n\n\\\n\n\n**THE STORY**\n\nLet $\\pi$ (\"pi\") be the proportion of U.S. adults that believe that climate change is real and caused by people. Thus $\\pi$ is some value between 0 and 1.\n\n\\\n\n\n\n## Exercise 1: Specify a prior model\n\nThe first step in learning about $\\pi$ is to specify a **prior model** for $\\pi$ (i.e. prior to collecting any data). Suppose your friend specifies their understanding of $\\pi$ through the \"Beta(2, 20)\" model. Plot this Beta model and discuss what it tells you about your friend's prior understanding. For example:\n\n- What do they think is the most likely value of $\\pi$?    \n- What range of $\\pi$ values do they think are plausible?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 2, beta = 20)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-3-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\n\n\\\n\\\n\n\n## Exercise 2: Check out some data\n\nThe second step in learning about $\\pi$, the proportion of U.S. adults that believe that climate change is real and caused by people, is to collect data. Your friend surveys 10 people and 6 believe that climate change is real and caused by people. The **likelihood function** of $\\pi$ plots the chance of getting this 6-out-of-10 survey result under different possible $\\pi$ values. Based on this plot:\n\n- With what values of $\\pi$ are the 6-out-of-10 results most consistent?\n- For what values of $\\pi$ would these 6-out-of-10 results be *unlikely*?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_binomial_likelihood(y = 6, n = 10)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-4-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\n\n\n\\\n\n\n\n## Exercise 3: Build the posterior model\n\nIn a Bayesian analysis of $\\pi$, we build a *posterior* model of $\\pi$ by combining the prior model of $\\pi$ with the data (represented through the likelihood function). Plot all 3 components below. Summarize your observations:\n\n- What's your friend's posterior understanding of $\\pi$?\n- How does their posterior understanding compare to their prior and likelihood? Thus how does their posterior *balance* the prior and data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta_binomial(alpha = 2, beta = 20, y = 6, n = 10)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-5-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\n\n\n\\\n\\\n\n\n\n## Exercise 4: Another friend\n\nConsider another friend that saw the same 6-out-of-10 polling data but started with a Beta(1, 1) prior model for $\\pi$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_beta(alpha = 1, beta = 1)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-6-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n- Describe the new friend's understanding of $\\pi$. Compared to the first friend, are they more or less sure about $\\pi$?\n\n- Do you think the new friend will have a different posterior model than the first friend? If so, how do you think it will compare?\n\n- Test your intuition. Use `plot_beta_binomial()` to explore your new friend's posterior model of $\\pi$.\n\n\n\n\\\n\\\n\n\n\n## Exercise 5: More data\n\nYour two friends come across more data. In the `pulse_of_the_nation` survey, 655 of 1000 people believed climate change was real and caused by people:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"pulse_of_the_nation\")\npulse_of_the_nation %>% \n  count(climate_change)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  climate_change                    n\n  <fct>                         <int>\n1 Not Real At All                 150\n2 Real and Caused by People       655\n3 Real but not Caused by People   195\n```\n:::\n:::\n\n\n- How do you think the additional data will impact your first friend's posterior understanding of $\\pi$? What about the second friend's?\n\n- Upon seeing the 1000-person survey results, do you think your two friends' posterior understandings of $\\pi$ will disagree a lot or a little?\n\n- Test your intuition. Use `plot_beta_binomial()` to explore both friends' posterior models of $\\pi$.\n\n\n::: {.cell}\n\n:::\n\n\n\n\\\n\\\n\n\n\n## Exercise 6: Your turn\n\nLet $\\pi$ be the proportion of U.S. adults that believe in ghosts.\n\n- Use `plot_beta()` to tune your own prior model of $\\pi$. To this end, think about what values of $\\pi$ you think are most likely and how sure you are. NOTE:    \n    - `alpha` and `beta` must be positive.\n    - The prior mean falls at `alpha` / (`alpha` + `beta`). Thus when `alpha` is smaller than `beta`, the prior mode falls below 0.5.\n    - In general, the smaller the `alpha` and `beta`, the more variable / less certain the prior.\n\n\n::: {.cell}\n\n:::\n\n\n\n- Collect some data. How many of the 1000 `pulse_of_the_nation` respondents believe in `ghosts`?\n\n\n\n::: {.cell}\n\n:::\n\n\n\n- Use `plot_beta_binomial()` to visualize your prior, data, and posterior.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\\\n\\\n\\\n\\\n\n\n\n\n# Part 2: Apply Bayesian thinking to a regression model\n\n**GOAL**\n\nThe above exercises applied Bayesian thinking to the analysis of a single parameter ($\\pi$) and a single \"data variable\" (Y). Let's extend this thinking to a regression model in which we can explore the *relationship* of Y with some predictors X.\n\n\n\\\n\n\n**THE STORY**    \n\nCapital Bikeshare is a bikeshare service in Washington, D.C. Our goal is to model the daily number of rides (Y) versus the temperature on that day (X).\n\n\n\\\n\n\n## Exercise 7: Simulate the prior models\n\nThough we can tune prior models to match our prior understanding, we'll start by utilizing default *weakly informative priors*. As a first step, use the code below to *simulate* the priors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the bikes data\ndata(bikes)\n\n# Simulate the prior model\nbike_prior <- stan_glm(\n  rides ~ temp_feel, data = bikes,\n  family = gaussian,\n  prior_PD = TRUE,\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000911 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 9.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.299 seconds (Warm-up)\nChain 1:                0.138 seconds (Sampling)\nChain 1:                1.437 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.697 seconds (Warm-up)\nChain 2:                0.102 seconds (Sampling)\nChain 2:                0.799 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.867 seconds (Warm-up)\nChain 3:                0.103 seconds (Sampling)\nChain 3:                0.97 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.7e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.405 seconds (Warm-up)\nChain 4:                0.121 seconds (Sampling)\nChain 4:                1.526 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\n`bike_prior` contains 20,000 prior plausible models of ridership by temperature. Plot just 200 of these models. Describe your observations. For example, do the weakly informative priors reflect a general prior *certainty* or a general prior *uncertainty* about the relationship between ridership and temperature?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 200 prior model lines\nbikes %>%\n  add_predicted_draws(bike_prior, n = 200) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n  geom_smooth(aes(y = .prediction, group = .draw), \n              size = 0.1, method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-13-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\n\n\\\n\n\n## Exercise 8: Collect some data\n\nNow that the priors are in place, check out the data. Plot and describe the observed relationship between `rides` and `temp_feel`.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\\\n\n\n## Exercise 9: Simulate the posterior\n\nNext, we can update our posterior understanding of the relationship between ridership and temperature by combining the prior and the data. To do so, **change ONE LINE** in the code below. (Currently, the code would simulate the prior model.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the posterior model\nbike_posterior <- stan_glm(\n  rides ~ temp_feel, data = bikes,\n  family = gaussian,\n  prior_PD = TRUE,\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.223 seconds (Warm-up)\nChain 1:                0.122 seconds (Sampling)\nChain 1:                1.345 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.628 seconds (Warm-up)\nChain 2:                0.101 seconds (Sampling)\nChain 2:                0.729 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.793 seconds (Warm-up)\nChain 3:                0.101 seconds (Sampling)\nChain 3:                0.894 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.318 seconds (Warm-up)\nChain 4:                0.112 seconds (Sampling)\nChain 4:                1.43 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\n\n\n\n\\\n\n\n\n\n## Exercise 10: Check the simulation\n\nThis simulation produces 20,000 posterior plausible models of the relationship between ridership and temperature. More specifically, they produce 20,000 pairs of possible intercepts and slopes. We can use these to **approximate** the actual posterior relationship (which is too complicated to specify mathematically). To understand whether our simulation results are \"trustworthy\", there are several visual and numerical diagnostics we might check. Consider just two.\n\nOur 20,000 posterior plausible models combine 4 separate \"chains\" or samples of 5,000 posterior plausible models each. **Trace plots** illustrate the *sequence* of the simulated `(Intercept)` and slope (`temp_feel`) values in these chains. We want these to look like random noise, suggesting that the simulation is stable and random-ish. How do they look to you?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(bike_posterior)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-16-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\nNext, we can check out density plots of the `(Intercept)` and slope (`temp_feel`) values in these chains. This gives us a sense for the distribution of values explored in our simulation, not just their sequence. We want the 4 density plots to look similar, suggesting that the simulation is stable (i.e. we could get a bigger simulation but the results wouldn't be much different). How do they look to you?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_dens_overlay(bike_posterior)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-17-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\n\\\n\n\n\n\n## Exercise 11: Examine the posterior\n\nWith some assurance that our simulation has stabilized, let's explore our posterior model. `bike_posterior` contains 20,000 posterior plausible models of ridership by temperature. Plot just 200 of these models. Describe your observations and how they compare to our prior understanding. HINT: See exercise 7 and add the observed data points.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\\\n\n\n\n## Exercise 12: Posterior summaries\n\nTo dig in deeper, we can examine the posterior models for the `(Intercept)` and `temp_feel` model coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Graphical summary of the posteriors\nmcmc_dens(bike_posterior)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-19-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\nWe can also obtain numerical summaries of these posteriors. Notice how these numerical summaries connect with the medians and ranges of the posterior models plotted above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Numerical summary of the posteriors\ntidy(bike_posterior, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept) 3463.       24696. -44839.     51853.\n2 temp_feel      0.636      353.   -690.       690.\n3 sigma       1078.        1598.     35.1     5865.\n```\n:::\n:::\n\n\nQuestions:\n\n- What does the posterior median `estimate` of the `temp_feel` coefficient (81.8) indicate about the relationship between ridership and temperature?\n\n- The 95% **posterior credible interval** for `temp_feel` suggests there's a 95% chance that this coefficient falls between 71.6 and 92.1. (This is not a typo! This is how interval estimates are interpreted in Bayesian analysis.) Based on this result, do you think we have enough evidence to conclude that ridership tends to increase as temperature increases?\n\n\n\n\n\n\\\n\n\n\n## Exercise 13: Posterior prediction\n\nIt's supposed to be 90 degrees tomorrow. Plot and discuss the posterior predictive model for how many rides we'll see tomorrow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate and plot the posterior predictive model\nset.seed(84735)\npredictions_90 <- posterior_predict(\n  bike_posterior, newdata = data.frame(temp_feel = 90))\nmcmc_areas(predictions_90)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-21-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\nWhat if it's supposed to be 70 degrees tomorrow? Plot and discuss the posterior predictive model for how many rides we'll see. How does this compare to our prediction for a 90-degree day?\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\\\n\n\n\n## Exercise 14: How accurate are the posterior predictions?\n\nIf we want to use our model to make such predictions, we should also question how accurate they might be. For illustration and computational purposes only, let's examine our model's posterior predictive performance for just 20 of our data points:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Take a random sample of 20 data points\nset.seed(84735)\nbikes_small <- bikes %>% \n  sample_n(size = 20)\n\n# Simulate posterior predictive models for these 20 data points\npredictions <- posterior_predict(bike_posterior, newdata = bikes_small)\n```\n:::\n\n\nThe plot below illustrates features of the posterior predictive models for the 20 data points:\n\n- the dark blue dots represent the *observed* data points\n- the light blue dots represent the corresponding posterior median *predicted* ridership for these data points\n- the long, thin blue lines represent the corresponding 95% prediction intervals for ridership\n- the shorter, darker blue lines represent the corresponding 50% prediction intervals for ridership\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppc_intervals(bikes_small$rides, yrep = predictions, x = bikes_small$temp_feel, \n\t\t\t\t\t\t\tprob = 0.5, prob_outer = 0.95) +\n\tlabs(x = \"temp_feel\", y = \"rides\")\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-24-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\nQuestions:\n\n- Roughly how far do the 20 data points fall from their corresponding posterior median predictions? For example, do they tend to be 10, 100, 1000, or 10000 rides off?\n- Roughly what percentage of the 20 data points fall within their corresponding 95% prediction intervals?\n- Do our posterior prediction models reflect general certainty or uncertainty about how many rides to expect on a given day?\n\n\n\n\n\\\n\n\n\n## Exercise 15: How accurate are the posterior predictions? Take two.\n\nThe `prediction_summary()` function provides numerical summaries of your observations in the plot above:\n\n- `mae` (median absolute error) measures the typical distance of an observed ridership outcome from its posterior predictive mean.\n- `mae_scaled` measures the typical number of standard deviations an observed ridership outcome from its posterior predictive mean.\n- `within_50` records the percentage of data points that fall within their 50% prediction intervals. `within_95` is similar but for the 95% prediction intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_summary(model = bike_posterior, data = bikes_small)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mae mae_scaled within_50 within_95\n1 1578.431  0.2772336         1         1\n```\n:::\n:::\n\n\nInterpret the 4 reported numbers and try to connect these to what you observed in the plot above.\n\nNOTE: `prediction_summary_cv()` provides a cross-validated alternative to measuring posterior prediction accuracy.\n\n\n\n\n\n\\\n\n\n\n\n## Exercise 16: You try. Tweak the prior.\n\nRecall that we utilized weakly informative priors in our analysis above. Instead, we could tune priors to reflect whatever prior information we might have about the relationship between ridership and temperature. For example, setting `prior = normal(100, 50)` indicates a prior understanding that, for each 1 degree increase in temperature, ridership likely increases by `100` riders though it could reasonably increase anywhere between 0 and 200 riders (100 - 2*50, 100 + 2*50).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the prior model\nnew_bike_prior <- stan_glm(\n  rides ~ temp_feel, data = bikes,\n  family = gaussian,\n  prior = normal(100, 50),\n  prior_PD = TRUE,\n  chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.258 seconds (Warm-up)\nChain 1:                0.141 seconds (Sampling)\nChain 1:                1.399 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.674 seconds (Warm-up)\nChain 2:                0.101 seconds (Sampling)\nChain 2:                0.775 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.84 seconds (Warm-up)\nChain 3:                0.113 seconds (Sampling)\nChain 3:                0.953 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.354 seconds (Warm-up)\nChain 4:                0.108 seconds (Sampling)\nChain 4:                1.462 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\n# 200 prior model lines\nbikes %>%\n  add_predicted_draws(new_bike_prior, n = 200) %>%\n  ggplot(aes(x = temp_feel, y = rides)) +\n  geom_smooth(aes(y = .prediction, group = .draw), \n              size = 0.1, method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](activity_files/figure-html/unnamed-chunk-26-1.png){fig-pos='center' width=432}\n:::\n:::\n\n\n\nYour turn. Tune and simulate priors that match the following scenarios.\n\n- We're pretty certain that, for each 1 degree increase in temperature, ridership tends to increase by somewhere between 90 and 110 riders.\n\n- We're uncertain about the relationship between ridership in temperature. For each 1 degree increase in temperature, ridership might typically decrease by as much as 100 degrees or increase by as much as 100 degrees.\n\n- We're pretty certain that, for each 1 degree increase in temperature, ridership tends to decrease.\n\n\n\n\n\\\n\n\n\n\n## Exercise 17: Weather\n\nApply what you've learned to build a Bayesian regression model 3pm temperature (`temp3pm`) by 9am temperature (`temp9am`) in Perth, Australia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(weather_perth)\n```\n:::\n\n\n\n\n\n\n\\\n\n\n\n## TO LEARN MORE\n\nChapters 9--11 in [Bayes Rules!](https://www.bayesrulesbook.com/chapter-9.html) go into more depth on Bayesian regression.\n",
    "supporting": [
      "activity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}